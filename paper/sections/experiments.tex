\paragraph{Design decisions.}
Although JAX already offers an experimental Taylor mode implementation, we re-implemented Taylor mode in PyTorch, taking heavy inspiration from the JAX implementation for the interface.
We decided so because PyTorch's \texttt{fx} library provides a user-friendly official interface to capture and transform compute graphs of functions to apply our proposed collapsing.
While we believe that such graph transformations should in principle be feasible in JAX as well (and could \eg be integrated into its \texttt{jit} compiler), it seems that this can currently only be achieved with (potentially fragile) internal APIs and requires a deep understanding of its internal tracing mechanisms.
Main advantage seems to be that we can clearly disentangle building the graph from simplifying it using linearity, whereas in JAX we could write a custom interpreter that would have to re-implement the logic of summing the $K$-th component for abitrary $K$.

\paragraph{Usage.} Our implementation takes a PyTorch function (\eg a neural net) and first captures its compute graph using \texttt{torch.fx}'s symbolic tracing mechanism, then replaces each operation with its Taylor arithmetic.
This yields the compute graph of the function's $K$-jet.
Users can use this vanilla Taylor mode to define their differential operator's computation.
The collapsing is done by again tracing said computation with \texttt{torch.fx} and rewriting the resulting graph, propagating the summation of highest coefficient up.
The resulting graph performs the same computation, but uses our collapsed Taylor mode.
See \Cref{fig:interface-overview} for a visual walk-through of the procedure.

\paragraph{Limitations.}
Two limitations of our implementation are that (i) it only supports a small number of primitives and missing primitives need to be implemented (as is the case for the Taylor mode implementation JAX) and (ii) that \texttt{torch.fx} graphs can, at the moment, not be \texttt{jit}-compiled with \texttt{torch.compile}, \ie our PyTorch experiments report both un-compiled Taylor mode and nested first-order AD, although the latter could be further accelerated.

\input{figures/torch_benchmark}
\input{tables/torch_benchmark}

\paragraph{Experimental procedure.}
We empirically validate our proposed collapsing approach using our PyTorch implementation of Taylor mode.
Since \texttt{torch.fx} is at the moment incompatible with \texttt{torch.compile}, all implementations are not \texttt{jit}-compiled.
We compare collapsed Taylor mode with vanilla Taylor mode and nested first-order AD on the previously discussed differential operators an NVIDIA RTX 6000 with 24 GiB memory.
To implement the (weighted) Laplacian and its stochastic counterpart, we use vector-Hessian-vector products (VHVPs) in forward-over-reverse order, as recommended by \citet{dagreou2024how}.
For the Bi-Laplacian, we simply nest two VHVPs.
For all implementations, we measure (i) run time, and peak memory when computing the (ii) differentiable operator (\eg required for PINNs), and (iii) the non-differentiable operator (\eg required for VMC).
The run time measurements commence with 10 warm-up runs, followed by 50 repetitions, and we report the smallest run time as best approximation to run time.
The peak memory allocation does not fluctuate, hence we determine it in a single run.

\paragraph{Experimental results.}
\Cref{fig:benchmark} visualizes the computational demands' growth \wrt the batch size (exact) and number of Monte-Carlo samples (stochastic) for fixed dimensions $D$.
We used a 5-layer MLP with $\tanh$ activation and around 3M parameters, similar to the ones employed in other works on PINNs \cite{dangel2024kroneckerfactored,shi2024stochastic}.
As the scalings with batch size and Monte-Carlo samples are both linear, we quantify the results by fitting linear functions and reporting their slopes (\ie time and memory added per datum/sample) in \Cref{tab:benchmark}. We make the following observations:
\begin{itemize}
\item \textbf{Collapsed Taylor mode accelerates standard Taylor mode.}
  When comparing collapsed with standard Taylor mode, we observe that the measure run time differences correspond well with the theoretical prediction.
  \Eg for the Laplacian, the ratio of propagated vectors between standard and collapsed is $\nicefrac{(2 + D)}{(1 + 2D)} \approx 0.5$; we observe that adding one datum adds $0.60ms$ to standard and $0.33ms$ to collapsed Taylor mode.
  Similar for the weighted Laplacian and for their stochastic versions.
  This means that we can compute these operators with twice the batch size or MC-samples in the same time as had we used standard Taylor mode.

  % TODO Could drop a sentence about the Bi-Laplacian, but the ratio of vectors is slightly complicated and may be confusing for readers
  % For the Bi-Laplacian, the ratio is $\nicefrac{4.5 D^2 - 1.5D + 6}{TODO}$

\item \textbf{Collapsed Taylor mode consistently outperforms nested 1\textsuperscript{st}-order AD.}
  For the exact and stochastic Laplacian and weighted Laplacian, collapsed Taylor mode is roughly twice as fast as nested AD while using 70-80\% of the latter's memory.
  The only exception is for the differentiable Bi-Laplacian, which consumes about 20\% more memory, but uses only 66\% time.
  For the randomized Bi-harmonic, we observe even greater run time and memory improvements by roughly 9 and 3 (or 6 if non-differentiable)x.

  \todo{FD: I don't have a good explanation the stochastic Bi-harmonic has the largest performance difference.
    I am using a similar implementation than for the exact Bi-harmonic.
    Would be cool if we had an explanation here.
    One main difference is that I have to \texttt{vmap} twice, first over the data, then over the vectors.
    It could be that this triggers some redundant re-computation, but is hard to verify.
  }

\item \textbf{If the PDE operator can be non-differentiable, both standard and Taylor mode require significantly less memory than nested 1\textsuperscript{st}-order AD.}
  This aligns with our understanding of Taylor mode, which performs a single forward propagation and does not need to store any intermediates for future differentiation, in contrast to the nested differentiation.
\end{itemize}
We also conducted experiments with JAX (+ \texttt{jit}) to rule out artifacts from the ML framework.
In general, we find that the choice of ML framework does not affect the results.
\Eg, for computing the Laplacian with nesting, we found that PyTorch increases by 0.61ms and 4.4 and 2.2 MiB per datum, while JAX increases by 0.57ms and 6.0, and 1.4 MiB per datum (\Cref{fig:vanilla-taylor-not-enough,tab:jax-benchmark}).
We find the same trend when comparing our collapsed Taylor mode and JAX's forward Laplacian.
Interestingly, we noticed that JAX's Taylor mode was consistently slower than our PyTorch implementation, despite using \texttt{jit}.
We conclude from these results that (both ours, as well as the existing JAX) Taylor mode implementations still have potential for improvements that may further improve the margin to nested first-order.

\begin{table}[!h]
  \centering
  \caption{\textbf{Comparison of theoretical and empirical performance ratios between standard and collapsed Taylor mode.}
    We write down how many vectors are added when adding another data point (exact) or another Monte-Carlo sample (stochastic).
    The ratio of vectors is an optimistic estimation of the performance ratio, because it assumes that all vectors are propagated independently (false), and that the required intermediates are TODO.
  }
  \begin{tabular}{cc|ccc}
    \toprule
    \textbf{Mode}
    & \makecell{\textbf{Add one datum} \\ \textbf{or MC sample}}
    & \makecell{\textbf{Laplacian} \\ ($D = 50$)}
    & \makecell{\textbf{Weighted Laplacian} \\ ($D=\rank(\mC)=50$)}
    & \makecell{\textbf{Bi-harmonic} \\ ($D=5$)}
    \\
    \midrule
    \multirow{4}{*}{\textbf{Exact}}
    & \textcolor{tab-orange}{$\Delta$ vectors (standard)}
    & $1 + 2D$
    & $1 + 2 \rank(\mC)$
    & $6D^2 - 2D + 1$
    \\
    & \textcolor{tab-green}{$\Delta$ vectors (collapsed)}
    & $2 + D$
    & $2 + \rank(\mC)$
    & $\nicefrac{9}{2} D^2 - \nicefrac{3}{2} D + 4$
    \\
    & Theoretical ratio $\nicefrac{\color{tab-green}\Delta}{\color{tab-orange}\Delta}$
    & \num{0.51}
    & \num{0.51}
    & \num{0.77}
    \\
    & Empirical time ratio
    & \num{0.55}
    & \num{0.55}
    & \num{0.88}
    \\
    & Empirical mem.\,ratio
    & \num{0.65}
    & \num{0.65}
    & \num{0.78}
    \\
    \midrule
    \multirow{4}{*}{\textbf{Stochastic}}
    & \textcolor{tab-orange}{$\Delta$ vectors (standard)}
    & $2$
    & $2$
    & $4$
    \\
    & \textcolor{tab-green}{$\Delta$ vectors (collapsed)}
    & $1$
    & $1$
    & $3$
    \\
    & Theoretical ratio $\nicefrac{\color{tab-green}\Delta}{\color{tab-orange}\Delta}$
    & \num{0.5}
    & \num{0.5}
    & \num{0.75}
    \\
    & Empirical time ratio
    & \num{0.54}
    & \num{0.54}
    & \num{0.76}
    \\
    & Empirical mem.\,ratio
    & \num{0.64}
    & \num{0.64}
    & \num{0.72}
    \\
    \bottomrule
  \end{tabular}
\end{table}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
