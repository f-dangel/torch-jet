Recently, neural networks have increasingly bee used to represent potentially high-dimensional probability densities or functions constrained by physical laws \cite{carleo2017solving, pfau2020ab, hermann2020deep, karniadakis2021physics, raissi2019physics, hu2023hutchinson, sun2020global}. The physics is typically encoded in the form of partial differential equations (PDEs), and the neural network representation must therefore satisfy such equations. This requires the application of partial differential operators to the neural network representation, which need to be computed with respect to the input rather than the trainable weights. Different to the setting of network training with first-order methods, these input derivatives are often high-order PDE operators, requiring second derivatives, or even more. Efficient automatic differentiation of such operators must therefore go beyond mere backpropagation, a realization that motivated this and previous work and is illustrated in Figure \ref{fig:benchmark-laplacians}.


\paragraph{Variational Monte Carlo} A prominent example is variational Monte Carlo (VMC), where neural quantum states must satisfy Schrödinger's equation \cite{carleo2017solving, pfau2020ab, hermann2020deep}. This involves the computation of the input Laplacian to capture the kinetic energy contribution to the Hamiltonian. In many settings, this Laplacian need not be differentiable with respect to the network weights \cite{li2023forward}. However, recent developments in optimization techniques have also revived the use of differentiable Laplacians \cite{webber2022rayleigh, toulouse2007optimization}.

\paragraph{Physics-Informed Neural Networks} A more general framework arises in physics-informed neural networks (PINNs), which cast PDE-constrained problems as optimization tasks by minimizing the residuals of the governing equations \cite{raissi2019physics, karniadakis2021physics}. This approach necessitates differentiable application of potentially high-order PDE operators. For instance, the PINN formulation of Kolmogorov-type equations—including the Fokker-Planck, Black-Scholes, and Hamilton-Jacobi-Bellman equations—requires the evaluation of weighted second-order input derivatives in high-dimensional spatial domains \cite{hu2023hutchinson, sun2024dynamical}.

\paragraph{Compuational Challenges} As illustrated in Figures \ref{fig:benchmark-laplacians} and Figure \ref{fig:benchmark-randomized-laplacians}, the computation of input derivatives can be a significant computational bottleneck in terms of both memory and runtime, particularly for high-dimensional or high-order operators. This limitation may restrict the scalability of recent approaches and hinder broader applicability. To address this challenge, we advocate for more efficient and automated use of advanced automatic differentiation techniques. In particular, we propose to generalize recent graph simplification strategies—such as the forward Laplacian scheme—by systematically exploiting the linearity of differential operators. We formulate these in terms of Taylor-mode automatic differentiation. This has the potential to both reduce resource consumption and broaden the range of feasible applications in VMC, PINNs, and related areas.

Our contributions are the following:
\begin{enumerate}
    \item We describe `collapsed' Taylor mode which allows to collapse the highest Taylor coefficients during the forward propagation whenever we compute a weighted sum of said derivatives.
    This trick has first been used in the `forward Laplacian' framework, which however does not rely on Taylor mode.
    Our collapsed Taylor mode improves over it in two ways.
    First, it separates the trick into using standard Taylor mode, combined with simple graph rewrites based on linearity.
    Second, it generalizes to other differential operators, and can also be applied to recently proposed randomization techniques for Taylor mode.

    \item This leads to a conceptually clean separation of concepts, allowing for a simple implementation that can handle all scenarios and does not require manual implementations for each differential operator.
    To demonstrate this, we build a Taylor mode library for PyTorch and equip it with the necessary graph simplifications using the \texttt{torch.fx} library, and show how collapsed Taylor mode accelerates the computation of various differential operators.

    \item Ideally, we find specially structured differential operators for which our trick is especially suited, and classify them.
    One example is the trace of derivative tensors of arbitrary order, although this seems to be an operator that is not practically relevant.
\end{enumerate}

Next tasks:
\begin{enumerate}
    \item @Felix: Run experiments with the weighted  sum of second-order derivatives with $\mC$ PSD to demonstrate we can go beyond the `classical' forward Laplacian

    \item @Felix: Look into computing the trace of third order derivatives.
    It will be interesting to check how first-order AD performs here.

    \item @Felix: Extend the graph simplifications to work with weighted sum of second-order derivatives where $\mC$ is indefinite.
    For this, we need to compute two collapsed 2-jets which share some computation (both use the same value on which the function is evaluated. They can therefore share this computation, as well as the derivatives).
    This will require some form of rudimentary common sub-expression elimination.

    \item @Felix [vague]: Think about an example where
\end{enumerate}
\newpage

\begin{figure}[t]
  \centering
\includegraphics*{../jet/exp/exp01_benchmark_laplacian/figures/architecture_tanh_mlp_768_768_512_512_1_device_cuda_dim_50_name_dangel2024kroneckerfactored_vary_batch_size.pdf}
\caption{ \textbf{Computing the Laplacian with collapsed Taylor mode is faster \emph{and} uses less memory than nested first-order AD; naive Taylor mode is competitive in time but uses more memory.}
  We compare different strategies for computing Laplacians for a $50\to 768 \to 768 \to 512 \to 512 \to 1$ MLP with tanh activations for varying batch sizes on an NVIDIA A40 GPU.
  Solid markers correspond to computing differentiable Laplacians (\eg used in PINNs), opaque markers compute a non-differentiable Laplacian (\eg used in VMC).
  Averaged over all batch sizes, \emph{our collapsed Taylor mode uses only 80\% memory of nested first-order AD (70\% in the non-differentiable setting), and 50\% of time}.
  In contrast, naive Taylor mode uses 120\% memory of nested first-order AD (70\% in the non-differentiable setting) and 95\% of time.
}
  \label{fig:benchmark-laplacians}
\end{figure}

\begin{figure}
  \centering
\includegraphics*{../jet/exp/exp01_benchmark_laplacian/figures/architecture_tanh_mlp_768_768_512_512_1_batch_size_2048_device_cuda_dim_50_distribution_normal_name_dangel2024kroneckerfactored_vary_num_samples.pdf}
\caption{\textbf{Collapsed Taylor mode is compatible with randomization.}
  We show the performance evaluation of the randomized Laplacian with identical setup \Cref{fig:benchmark-laplacians}, but we fix the batch size to be $2048$ and vary the number of samples $S$ used by the Monte-Carlo estimator.
  We find the same relative speedups and memory reductions of collapsing Taylor mode than for the exact Laplacian (as expected since all expressions for the computational demands look the same, but with $S$ replacing $D$).
  Also compare to \Cref{fig:benchmark-laplacians} and note how for $N=2048$ we can obtain a noisy estimate of the Laplacian at much lower cost when $S < D$.
}\label{fig:benchmark-randomized-laplacians}
\end{figure}

\begin{figure}
  \centering
\includegraphics*{../jet/exp/exp01_benchmark_laplacian/figures/architecture_tanh_mlp_768_768_512_512_1_batch_size_16_device_cuda_name_dangel2024kroneckerfactored_bilaplacian_vary_dim.pdf}
\caption{\textbf{Collapsed Taylor mode applied to the Bi-Laplacian.}
  The net is the same as in \Cref{fig:benchmark-laplacians}, but we fix the batch size to be $16$ and vary the net's input dimension.
  Note that nesting first-order AD scales worse.
}\label{fig:benchmark-randomized-laplacians}
\end{figure}


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
