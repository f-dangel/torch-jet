We start with a self-contained introduction of Taylor-mode automatic differentiation~\citep[][Chapter 13]{griewank2008evaluating}.

\paragraph{Taylor mode automatic differentiation (scalar case).}
For simplicity, consider a scalar-valued function $f: \sR \to \sR, x \mapsto f(x)$, and a path $x(t): [-\tau, \tau] \to \sR$.
We will later introduce notation to seamlessly handle tensor-valued functions.
Assume we Taylor-expand the path in input space up to order $K$, i.e.\,$x(t) \approx \sum_{k=0}^K\nicefrac{t^k}{k!}
\left.\nicefrac{\partial^k x(t)}{\partial t^k}\right|_{t=0} \coloneqq \sum_{k=0}^K \nicefrac{t^k}{k!}
x_k$ with $x_k \coloneqq \left.\nicefrac{\partial^k f(x)}{\partial t^k}\right|_{t=0}$. This operation of creating a truncated Taylor approximation is called a \emph{$K$-jet}.
Our goal is to compute the $K$-jet of $f(x(t))$, given the $K$-jet of $x(t)$.
In notation, we want to compute $f_k \coloneqq \left.\nicefrac{\partial f(x(t))}{\partial t^k}\right|_{t=0}$ so that $f(x(t)) \approx \sum_{k=0}^K \nicefrac{t^k}{k!} f_k$:
\begin{center}
  \begin{tikzpicture}
    \node[align=center] (topleft) {Path in input space \\
      $x(t)$, $t \in [-\tau, \tau]$};

    \node[align=center, right=4cm of topleft] (topright) {Path in output space \\ $f(x(t))$};
    \draw [-Latex] (topleft.east) to node [midway, above] {$f$} (topright.west);

    \node[align=center, below=1cm of topright] (bottomright) {Truncated Taylor series
      \\
      $f(x(t)) \approx \sum_{k=0}^K \frac{t^k}{k!} f_k$
      \\
      {\color{maincolor}$(f_0, \dots, f_K)$}
    };
    \draw [-Latex] (topright.south) to node [midway, right] {$K$-jet} (bottomright.north);

    \node[align=center, below=1cm of topleft] (bottomleft) {Truncated Taylor series
      \\
      $x(t) \approx \sum_{k=0}^K \frac{t^k}{k!} x_k$
      \\
      {\color{maincolor} $(x_0, \dots, x_K)$}};
    \draw [-Latex] (topleft.south) to node [midway, left] {$K$-jet} (bottomleft.north);

    \draw [-Latex, maincolor] (bottomleft.east) to node [midway, above, maincolor] {Taylor mode} (bottomright.west);
  \end{tikzpicture}
\end{center}

Let's start by writing out the first two derivatives, for which we need the chain and product rules (for notational convenience, we omit the derivative evaluations at $t=0$):
\begin{subequations}\label{eq:taylor-mode-scalar}
  \begin{align}
    f_0
    &=
      f(x_0)
    \\
    f_1
    &=
      \frac{\partial f}{\partial x} \frac{\partial x}{\partial t}
      =
      \frac{\partial f}{\partial x} x_1
    \\
    f_2
    &=
      \frac{\partial^2 f}{\partial x^2} \left( \frac{\partial x}{\partial t} \right)^2
      +
      \frac{\partial f}{\partial x} \frac{\partial^2 x}{\partial t^2}
      =
      \frac{\partial^2 f}{\partial x^2} x_1^2
      +
      \frac{\partial f}{\partial x} x_2
    \\
    f_3
    &=
      \frac{\partial^3 f}{\partial x^3} \left( \frac{\partial x}{\partial t} \right)^3
      +
      3 \frac{\partial f^2}{\partial x^2} \frac{\partial x}{\partial t}
      \frac{\partial^2 x}{\partial t^2}
      +
      \frac{\partial f}{\partial x} \frac{\partial^3 x}{\partial t^3}
      =
      \frac{\partial^3 f}{\partial x^3} x_1^3
      +
      3 \frac{\partial^2 f}{\partial x^2} x_1 x_2
      +
      \frac{\partial f}{\partial x} \frac{\partial^3 x}{\partial t^3}
      \intertext{At this point, we can start to see a pattern.
      The incoming Taylor coefficients $\{x_k\}$ are processed into the outgoing Taylor coefficients $\{f_k\}$ using derivatives of $f$.
      And the coefficient $f_k$ use only incoming derivatives up to that order, $\{x_{k'<k}\}$.
      The number of terms grows with $k$, and we can see that the incoming Taylor coefficients are chosen from the \emph{integer partitioning of $k$}, for instance $\partitioning(3) = \{ \{1,1,1\}, \{1, 2\}, \{3\} \}$.
      Finally, there is a also a multiplicity term that depends on the set of from the integer partitioning.
      In total, this gives the well-known Fa\`a di Bruno formula~\cite{arbogast1800calcul,hardy2006combinatorics,faa1857note}
      }
      \label{eq:faa-di-bruno}
      f_k
    &=
      \sum_{\sigma \in \partitioning(k)}
      \nu(\sigma) \frac{\partial^{|\sigma|} f}{\partial x^{|\sigma|}}
      \prod_{s \in \sigma} x_s
      \quad
      \text{with multiplicity}
      \quad
      \nu(\sigma)
      =
      \frac{k!}{
      \left(\prod_{s \in \sigma} n_s!\right)
      \left(\prod_{s \in \sigma} s!\right)
      }
  \end{align}
\end{subequations}
where $n_s$ counts the occurrences of $s$ in an integer partitioning $\sigma$, for instance $n_1(\{1,1,3\}) = 2$.

\paragraph{The vector/matrix/tensor-valued case.}
So far, we assumed a scalar-to-scalar function.
To generalize of \cref{eq:faa-di-bruno} to higher dimensions, we will introduce the notation $\partial^kf[\vx_{i_1}, \dots, \vx_{i_k}]$ to indicate the contraction of the tensor containing the $k$-th order partial derivatives with the vectors $\vx_{i_1}, \dots \vx_{i_k}$.
With this notation, we can seamlessly write (with $\vx$ and $f(\vx)$ being vectors now)
\begin{align}
  \vf_k
  &=
    \sum_{\sigma \in \partitioning(k)} \nu(\sigma) \partial^{|\sigma|} f[\tensorprod{s \in \sigma} \vx_s]
\end{align}

\paragraph{Function composition.} Assume now that $f = g \circ h$. Then, Taylor mode proceeds as follows
\begin{align}\label{eq:taylor-mode-composition}
  \begin{pmatrix*}
    \vx_0
    \\
    \vx_1
    \\
    \vx_2
    \\
    \vdots
    \\
    \vx_K
  \end{pmatrix*}
  &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
    \begin{pmatrix*}[l]
      \vh_0 =  h(\vx_0)
      \\
      \vh_1 = \partial h[\vx_1]
      \\
      \vh_2 = \partial^2 h[\vx_1, \vx_1] + \partial h[\vx_2]
      \\
      \vdots
      \\
      \vh_K =
      \displaystyle\sum_{\mathclap{\sigma \in \partitioning(K)}} \nu(\sigma) \partial^{|\sigma|} h[\tensorprod{s \in \sigma} \vx_s]
    \end{pmatrix*}
    \overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
    \begin{pmatrix*}[l]
      \vg_0 =  g(\vh_0)
      \\
      \vg_1 = \partial g[\vh_1]
      \\
      \vg_2 = \partial^2 g[\vh_1, \vh_1] + \partial g[\vh_2]
      \\
      \vdots
      \\
      \vg_K =
      \displaystyle\sum_{\mathclap{\sigma \in \partitioning(K)}} \nu(\sigma) \partial^{|\sigma|} g[\otimes_{s \in \sigma} \vh_s]
    \end{pmatrix*}
    \overset{\text{(\ref{eq:taylor-mode-scalar})}}{=}
    \begin{pmatrix*}[l]
      \vf_0 =  f(\vx_0)
      \\
      \vf_1 = \partial f[\vx_1]
      \\
      \vf_2 = \partial^2 f[\vx_1, \vx_1] + \partial f[\vx_2]
      \\
      \vdots
      \\
      \vf_K =
      \displaystyle\sum_{\mathclap{\sigma \in \partitioning(K)}} \nu(\sigma) \partial^{|\sigma|} f[\tensorprod{s \in \sigma} \vx_s]
    \end{pmatrix*}
\end{align}
This equation tells us how to (i) propagate forward Taylor coefficients through a directed acyclic compute graph (DAG) that represents a chain of function compositions, and (ii) how the results relate to the composition's derivative \wrt the input space.


\subsection{Computing Laplacians}
Taylor mode from the previous sub-section serves as backbone for computing many differential operators like the Laplacian.
The fundamental problem is to figure out how to set the jet degrees $K$ and the Taylor coefficients $x_k$.

As an example, let's consider computing the Laplacian of a function $f: \sR^D \to \sR$,
\begin{align}
  \Delta f(\vx)
  =
  \sum_{d=1}^D \frac{\partial^2f(\vx)}{\partial x_d^2}
  =
  \sum_{d=1}^D \partial^2f[\ve_d, \ve_d]
\end{align}
Pattern matching with \cref{eq:taylor-mode-composition}, we see that one way to compute the Laplacian is to set $K=2$, $\vx_0 = \vx$, and then compute the forward-propagated coefficients of $\{(\vx_{1,d} = \ve_d, \vx_{2,d} = \vzero)\}_d$.
\begin{align}\label{eq:laplacian-naive}
  \begin{split}
    \begin{pmatrix*}
      \vx_0
      \\
      \{\vx_{1,d} \}
      \\
      \{\vx_{2,d} \}
    \end{pmatrix*}
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vh_0 =  h(\vx_0)
        \\
        \{\vh_{1,d}\} = \{\partial h[\vx_{1,d}]\}
        \\
        \{\vh_{2,d}\} = \{\partial^2 h[\vx_{1,d}, \vx_{1,d}] + \partial h[\vx_{2,d}]\}
      \end{pmatrix*}
    \\
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vg_0 =  g(\vh_0)
        \\
        \{\vg_{1,d}\} = \{\partial g[\vh_{1,d}]\}
        \\
        \{\vg_{2,d}\} = \{ \partial^2 g[\vh_{1,d}, \vh_{1,d}] + \partial g[\vh_{2,d}] \}
      \end{pmatrix*}
      \overset{\text{(\ref{eq:taylor-mode-scalar})}}{=}
      \begin{pmatrix*}[l]
        \vf_0 =  f(\vx_0)
        \\
        \{\vf_{1,d} \} = \{ \partial f[\vx_{1,d}] \}
        \\
        \{ \vf_{2,d} \} = \{ \partial^2 f[\vx_{1,d}, \vx_{1,d}] + \partial f[\vx_{2,d}] \}
      \end{pmatrix*}
    \\
    &\overset{\text{slice}}{\to} \{ \vg_{2,d} \}
    \\
    &\overset{\text{sum}}{\to} \sum_{d=1}^D \{ \vg_{2,d} \}
      \overset{\{(\vx_{1,d} = \ve_d, \vx_{2,d} = \vzero)\}_d}{=} \Delta f(\vx_0)
  \end{split}
\end{align}
In this scheme, we push forward $1 + D + D$ coefficients.
Can we do better?

\subsection{Using Linearity---Recovering the Forward Laplacian}
Looking at the scheme in \cref{eq:laplacian-naive}, we can realize that we can propagate the summation up the computation graph. To do that, we can use the linearity of $\partial g[\bullet]$.
Propagating the summation up the computation graph, we obtain (changes highlighted in \textcolor{maincolor}{color}):
\begin{align}\label{eq:laplacian-efficient}
  \begin{split}
    \begin{pmatrix*}
      \vx_0
      \\
      \{\vx_{1,d} \}
      \\
      \mathemph{\displaystyle\sum_{d=1}^D \vx_{2,d}}
    \end{pmatrix*}
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vh_0 =  h(\vx_0)
        \\
        \{\vh_{1,d}\} = \{\partial h[\vx_{1,d}]\}
        \\
        \mathemph{\displaystyle\sum_{d=1}^D \vh_{2,d}} = \displaystyle\sum_{d=1}^D \partial^2 h[\vx_{1,d}, \vx_{1,d}] + \partial h[\mathemph{\displaystyle\sum_{d=1}^D\vx_{2,d}}]
      \end{pmatrix*}
    \\
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vg_0 =  g(\vh_0)
        \\
        \{\vg_{1,d}\} = \{\partial g[\vh_{1,d}]\}
        \\
        \mathemph{\displaystyle\sum_{d=1}^D\{\vg_{2,d}\}} = \displaystyle\sum_{d=1}^D \partial^2 g[\vh_{1,d}, \vh_{1,d}] + \partial g[\mathemph{\displaystyle\sum_{d=1}^D\vh_{2,d}}]
      \end{pmatrix*}
      \overset{\text{(\ref{eq:taylor-mode-scalar})}}{=}
      \begin{pmatrix*}[l]
        \vf_0 =  f(\vx_0)
        \\
        \{\vf_{1,d} \} = \{ \partial f[\vx_{1,d}] \}
        \\
        \displaystyle\sum_{d=1}^D \vf_{2,d} = \displaystyle\sum_{d=1}^D \partial^2 f[\vx_{1,d}, \vx_{1,d}] + \partial f[\vx_{2,d}]
      \end{pmatrix*}
    \\
    &\overset{\text{slice}}{\to} \mathemph{\sum_{d=1}^D \{ \vg_{2,d} \}}
      \overset{\{(\vx_{1,d} = \ve_d, \vx_{2,d} = \vzero)\}_d}{=}
      \Delta f(\vx_0)
  \end{split}
\end{align}
In this scheme, the second-order coefficients are \emph{first summed, then propagated}, \ie we push forward $1 + D + 1$ vectors instead of $1 + D + D$.
In fact, this scheme in \cref{eq:laplacian-efficient} corresponds to the \emph{forward Laplacian} framework from \citet{li2023forward}.
However, we recovered it from a different starting point: by using Taylor mode in combination with sum propagations.

\begin{table}[h]
    \centering
    \begin{tabular}{lcc|cc}
        \toprule
        \multirow{2}{*}{Operation} & \multicolumn{2}{c|}{Taylor arithmetic (TA)} & \multicolumn{2}{c}{Laplacian arithmetic (LA)} \\
        & Multiplications & Additions & Multiplications & Additions \\
        \midrule
        $\cos$  & $4D$  & $D$   & $3D + 1$  & $D + 1$ \\
        $\sin$  & $4D$  & $D$   & $3D + 1$  & $D + 1$ \\
        $\exp$  & $3D$  & $D$   & $2D + 1$  & $D + 1$ \\
        \midrule
        $\text{mult}$ & $5D$  & $3D$  & $3D + 2$  & $D + 2$ \\
        $\text{add}$  & $0$   & $2D$  & $D (0?)$  & $1$ \\
        \bottomrule
    \end{tabular}
    \caption{Number of multiplications and additions for atomic operations in Taylor Arithmetic (TA) and Laplacian Arithmetic (LA).}
    \label{tab:arithmetics}
\end{table}


\paragraph{Weighted sums of second-order derivatives.}
We can generalize the scheme in \cref{eq:laplacian-efficient} to computing differential operators of the form $\sum_{i,j} C_{i,j} \nicefrac{\partial^2f}{\partial \evx_i \evx_j}$ with a symmetric coefficient matrix $\mC \in \sR^{D\times D}$. For simplicity, let's first assume that $\mC$ is positive semi-definite, which means we can express it as $\mC = \sum_{d=1}^{\rank(\mC)} \vv_d \vv_d^\top$ with real-valued $\vv_d \in \sR^D$, and therefore
\begin{align}\label{eq:weighted-laplacian}
  \sum_{i,j} C_{i,j} \frac{\partial^2f}{\partial x_i \partial x_j}
  =
  \sum_{d=1}^{\rank(\mC)} \partial^2f[\vv_d, \vv_d]\,.
\end{align}
This can be computed using \cref{eq:laplacian-naive}, and the simplified version in \cref{eq:laplacian-efficient}, by setting $\vx_{1,d} = \vv_d, \vx_{2,d} = \vzero$.
Also here, the second-order coefficients can be collapsed, and instead of pushing forward $1 + 2 \rank(\mC)$ vectors, we are left with $2 + \rank(\mC)$ vectors.

If $\mC$ is in-definite, then some $\vv_d$ will be purely complex, \ie have non-zero imaginary but zero real part, and some will be purely real (alternatively, we can stick to real-valued arithmetic, but incorporate a minus sign for the vectors associated with negative eigenvalues of $\mC$; it is currently unclear if we even need this: there might not be relevant use cases for $\mC$ indefinite).
We can either compute the differential operator in complex-valued arithmetic, or split the positive and negative parts of the spectrum into two separate collapsed jets.

\todo{We can make the connection to \cite{hu2023hutchinson} equation (5) clear and discuss the case of $\mC$ being a function of the input}

\paragraph{Partially collapsed Taylor mode.}
A use case for such partially collapsed jets is in \cite{hu2023hutchinson}, which uses randomized Taylor mode (see \cref{subsec:randomized-taylor-mode}) to estimate the Laplacian.
In their application, the randomized Laplacian feeds into square loss.
While the Laplacian estimator itself is unbiased, squaring it in the loss function introduces a bias. 
Obtaining an unbiased estimator of the loss requires computing two randomized Laplacians using different sets of random vectors. 
We could handle this partial collapse and its propagation up the computation graph with our graph simplifications.
However, \cite{hu2023hutchinson} do not use this unbiased version in practice. 
They argue it has too much variance.

\subsection{Using Linearity in Randomization Taylor Mode}\label{subsec:randomized-taylor-mode}
\citep{shi2024stochastic} introduce randomization into Taylor mode, similar to stochastic backpropagation~\cite{oktay2021randomized}.
This lowers the computational cost when $D$ is prohibitively large for exact evaluation of differential operators.
Take the weighted Laplacian from \Cref{eq:weighted-laplacian} and assume we have access to random vectors $\rvv$ with and $\E[\vv \vv^{\top}] = \mC$.
Then we can draw $S \ll D$ random vectors $\vv_1, \vv_2, \dots, \vv_S \overset{\text{i.i.d.}}{\sim} \rvv$ and compute
\begin{align}
  \begin{split}
    \E[\partial^2f[\rvv, \rvv]]
    &=
      \E \left[
      \rvv^{\top} \frac{\partial^2 f}{\partial \vx \partial \vx} \rvv
      \right]
      =
      % \E \left[
      %   \sum_{i,j} \ervv_i \frac{\partial^2 f}{\partial x_i \partial x_j} \ervv_j
      % \right]
      % =
      \sum_{i,j} \frac{\partial^2 f}{\partial x_i \partial x_j}
      \E \left[
      \ervv_i  \ervv_j
      \right]
      =
      \sum_{i,j} \frac{\partial^2 f}{\partial x_i \partial x_j} C_{i,j}
    \\
    &\approx
      \frac{1}{S}
      \sum_{s=1}^S
      \partial^2 f[\vv_s, \vv_s]\,.
  \end{split}
\end{align}
Again, we see that the second-order coefficients can be collapsed when setting $\vx_{1,d} = \vv_d, \vx_{2,d} = \vzero$ in \cref{eq:laplacian-naive,eq:laplacian-efficient}.

\subsection{Using Linearity in Higher-Order Operators}
\paragraph{Traces of higher-order derivative tensors}
We can view the Laplacian as trace of the Hessian (the tensor of second-order derivatives).
A natural generalization to higher-order derivatives would be the tensor trace operator
\begin{align}\label{eq:trace-differential-operator}
    \Tr( \partial^K f(\vx) )
    \coloneqq
    \sum_{d=1}^D
    \frac{\partial^K f(\vx)}{\partial \evx_d^K}
    =
    \sum_{d=1}^D
    \partial^K f(\vx) [\ve_d, \ve_d, \dots, \ve_d]
\end{align}
which reduces to the Laplacian for $K=2$.
To compute this operator naively with Taylor mode, we have to propagate $1 + D K$ coefficients, then sum the coefficients of order $K$.
This sum can be pulled inside the forward propagation, similar to the forward Laplacian, resulting in $1 + D (K - 1) + 1$ coefficients.
While the improvements of this trick diminish as $K$ grows, the savings can still be significant for $K = 3, 4$.
At the moment, I do not know a relevant application for operators of the form \cref{eq:trace-differential-operator}.

\paragraph{Biharmonic operator} The biharmonic operator is a fourth-order differential operator that is interesting for some practitioners.
It is defined as
\begin{align}
    \Delta^2 f(\vx)
    \coloneqq
    \sum_{i=1}^D \sum_{j=1}^D
    \frac{\partial^4 f(\vx)}{\partial \evx_i^2 \partial \evx_j^2}
    =
    \sum_{i=1}^D \sum_{j=1}^D
    \partial^4 f(\vx) [\ve_i, \ve_i, \ve_j, \ve_j]
\end{align}
\citet{hu2023hutchinson} propose to approximate it in an unbiased fashion via
\begin{align}
    \Delta^2 f(\vx)
    \coloneqq
    \frac{1}{3}
    \E_{\rvv \sim \gN(\vzero, \mI)} \left[
    \partial^4 f(\vx) [\rvv, \rvv, \rvv, \rvv]
    \right]
    \qquad 
\end{align}
If we want to estimate this quantity using multiple samples, we can again pull the summation (from averaging over examples) into the Taylor mode propagation, resulting in collapsed Taylor mode.
I think we can improve their results from Table 3 in terms of memory and run time.

\paragraph{Using Linearity --- Biharmonic Operator}

When writing out the Biharmonic operator, applied to $g \circ h$
\begin{align}
\sum_{i=1}^d \sum_{j=1}^d
\frac{\partial^4}{\partial x_i^2 \partial x_j^2}
(g\circ h)(x) 
&= 
\sum_{i=1}^d \sum_{j=1}^d \Big[
\partial^4 g(h(x))[h_i^2(x) h_j(x)^2 \\
&+ 
\partial^3 g(h(x)) \left[h_i(x)^2 h_{jj}(x)
+ 
4 h_i(x) h_j(x) h_{ij}(x) 
+ 
h_{ii}(x) h_j(x)^2
\right] \\
&+ 
\partial^2 g(h(x)) \left[
2 h_{ij}(x)^2 
+ 
2 h_i(x) h_{ijj}(x) 
+ 
2 h_{iij}(x) h_j(x) 
+ 
h_{ii}(x) h_{jj}(x)
\right] \\
&+
\partial g(h(x)) h_{iijj}(x)\Big] \\
\end{align}
 we see several terms which occur only as sums:
 \begin{align}
&\sum_{i=1}^d \sum_{j=1}^d 
\partial^4 g(h(x))
[h_i^2(x) h_j(x)^2] \\
&+ 
\partial^3 g(h(x))\left[
\sum_{i=1}^d\left(
h_i(x)^2 
\mathemph{\sum_{j=1}^d  h_{jj}(x)}
\right) 
+
4 \sum_{i=1}^d \sum_{j=1}^d \left(
h_i(x) h_j(x) h_{ij}(x)
\right)
+
\sum_{j=1}^d \left(
\mathemph{\sum_{i=1}^d  h_{ii}(x)} h_j(x)^2
\right) 
\right] \\
&+
\partial^2 g(h(x)) \left[
\sum_{i=1}^d \sum_{j=1}^d \left(
2 h_{ij}(x)^2 
\right) 
+
2 \sum_{i=1}^d \left(
h_i(x) \mathemph{\sum_{j=1}^d  h_{ijj}(x)}
\right) 
+
2 \sum_{j=1}^d \left(
\mathemph{\sum_{i=1}^d  h_{iij}(x)} h_j(x) 
\right) 
+
\mathemph{\sum_{i=1}^d h_{ii}(x) \sum_{j=1}^d  h_{jj}(x)}
\right] \\
&+ 
\partial g(h(x)) \mathemph{\sum_{i=1}^d \sum_{j=1}^d h_{iijj}(x)}
\Big].
\end{align}
\todo{Do we assume continuity of all derivatives? Required for equivalence of third order sum} Leveraging the fact that there are equivalent sums $\sum_j h_{jj} = \sum_i h_{ii}$ and $\sum_j h_{ijj} = \sum_i h_{iij}$ as well as sums like $\displaystyle \sum_{i=1}^d  \sum_{j=1}^d \partial^3 \vh [
        \vx_i, \vx_i, \vx_{jj}]$ are the same as $\displaystyle \sum_{i=1}^d  \sum_{j=1}^d \partial^3 \vh [\vx_{ii},
        \vx_j, \vx_j]$
        everything is combined in the following efficient propagation scheme

\begin{align}\label{eq:biharm-efficient}
  \begin{split}
    \begin{pmatrix*}
      \vx_0
      \\
      \{\vx_{i} \}
      \\
      \{\vx_{ij} \}
      \\
      \mathemph{\displaystyle\sum_{i=1}^D \vx_{ii}}  
      \\
      \mathemph{\displaystyle\sum_{i=1}^D \vx_{iij}}
      \\
      \mathemph{\displaystyle\sum_{i=1}^D \sum_{j=1}^D  \vx_{iijj}}
    \end{pmatrix*}
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vh_0 =  \vh(\vx_0)
        \\
        \{\vh_{i}\} = \{\partial \vh[\vx_{i}]\}
        \\
        \mathemph{\displaystyle \sum_{i=1}^D \vh_{ii}} = \displaystyle\sum_{i=1}^D \partial^2 \vh[
        \vx_{i}, \vx_{i}]
        + \partial \vh[\mathemph{\displaystyle\sum_{i=1}^D\vx_{ii}}]
        \\
        \mathemph{\displaystyle \sum_{i=1}^D \vh_{iij}} = 
        \displaystyle\sum_{i=1}^D \left(
        \partial^3 \vh[
        \vx_{i}, \vx_{i}, \vx_{j}] 
        + 
        2 \partial^2 \vh[
        \vx_{i, j}, \vx_{i}] \right)
        + 
        \partial^2 \vh[
        \vx_{j}, \mathemph{\displaystyle\sum_{i = 1}^d\vx_{ii}}]
        + 
        \partial \vh[
        \mathemph{\displaystyle\sum_{i=1}^D\vx_{iij}}]
        \\
        \mathemph{\displaystyle\sum_{i=1}^d \sum_{j=1}^d \vh_{iijj}} = 
        \displaystyle\sum_{i=1}^d \sum_{j=1}^d
        \partial^4 \vh
        [\vx_i, \vx_i, \vx_j, \vx_j] 
        + 
        2 \displaystyle \sum_{j=1}^d \partial^3 \vh [
        \vx_j, \vx_j, \mathemph{\displaystyle \sum_{i=1}^d \vx_{ii}} ]
        \\
        +
        4 \displaystyle \sum_{i=1}^d \sum_{j=1}^d \partial^3 \vh [
        \vx_i, \vx_j, \vx_{ij}]
        +
        \displaystyle 2\sum_{i=1}^d \sum_{j=1}^d \partial^2 \vh [
         \vx_{ij}, \vx_{ij}]
        +
        \displaystyle 4\sum_{j=1}^d \partial^2 \vh [
        \vx_j, \mathemph{\sum_{i=1}^d  \vx_{iij}}]
        \\
        +
        \displaystyle \partial^2 \vh [
        \mathemph{\sum_{i=1}^d \vx_{ii}, \sum_{i=1}^d \vx_{ii}}]
        + 
        \partial \vh [\mathemph{\sum_{i=1}^d \sum_{j=1}^d \vx_{iijj}}]
        \Big]
      \end{pmatrix*}
    \\
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
\begin{pmatrix*}[l]
        \vg_0 =  \vg(\vh_0)
        \\
        \{\vg_{i}\} = \{\partial \vg[\vh_{i}]\}
        \\
        \mathemph{\displaystyle \sum_{i=1}^D \vg_{ii}} = \displaystyle\sum_{i=1}^D \partial^2 \vg[
        \vh_{i}, \vh_{i}]
        + \partial \vg[\mathemph{\displaystyle\sum_{i=1}^D\vh_{ii}}]
        \\
        \mathemph{\displaystyle \sum_{i=1}^D \vg_{iij}} = 
        \displaystyle\sum_{i=1}^D \left(
        \partial^3 \vg[
        \vh_{i}, \vh_{i}, \vh_{j}] 
        + 
        2 \partial^2 \vg[
        \vh_{ij}, \vh_{i}] \right)
        + 
        \partial^2 \vg[
        \vh_{j}, \mathemph{\displaystyle\sum_{i=1}^d\vh_{ii}}]
        + 
        \partial \vg[
        \mathemph{\displaystyle\sum_{i=1}^D\vh_{iij}}]
        \\
        \mathemph{\displaystyle\sum_{i=1}^d \sum_{j=1}^d \vg_{iijj}} = 
        \displaystyle\sum_{i=1}^d \sum_{j=1}^d
        \partial^4 \vg
        [\vh_i, \vh_i, \vh_j, \vh_j] 
        + 
        2 \displaystyle \sum_{j=1}^d \partial^3 \vg [
        \vh_j, \vh_j, \mathemph{\displaystyle \sum_{i=1}^d \vh_{ii}} ]
        \\
        +
        4 \displaystyle \sum_{i=1}^d \sum_{j=1}^d \partial^3 \vg [
        \vh_i, \vh_j, \vh_{ij}]
        +
        \displaystyle 2\sum_{i=1}^d \sum_{j=1}^d \partial^2 \vg [
         \vh_{ij}, \vh_{ij}]
        +
        \displaystyle 4\sum_{j=1}^d \partial^2 \vg [
        \vh_j, \mathemph{\sum_{i=1}^d  \vh_{iij}}]
        \\
        +
        \displaystyle \partial^2 \vg [
        \mathemph{\sum_{i=1}^d \vh_{ii}, \sum_{i=1}^d \vh_{ii}}]
        + 
        \partial \vg [\mathemph{\sum_{i=1}^d \sum_{j=1}^d \vh_{iijj}}]
        \Big]
      \end{pmatrix*}
    \\
    \\
    &\overset{\text{slice}}{\to} \mathemph{\sum_{i=1}^D \sum_{i=1}^D \{ \vg_{iijj} \}}
      \overset{\{\frac{d}{d t} \vx = I_D, \frac{d^2}{d t^2} \vx = 0, \frac{d^3}{d t^3} \vx = 0, \frac{d^4}{d t^4} \vx = 0\}}{=}
      \Delta^ 2f(\vx_0)
  \end{split}
\end{align}
\paragraph{Using Linearity --- General Differential Operators}

Let a general differential operator be given as 
\begin{equation}
    D = \sum_{i = 1}^N D_i
\end{equation}
where every $D_i$ is itself
\begin{equation}
    D_i = \sum_{j=1}^{N_i} \frac{\partial^{k^j}}{\partial x_1^{k_1^j} \dots \partial x_d^{k_d^j}}
\end{equation}
and $k^j = \sum_{l = 1}^{d} k_l^j$.
Consider $D_i$ applied to $g \circ h$ 
\begin{equation}
    D_i(g \circ h) =  \sum_{j=1}^{N_i} \frac{\partial^{k^j}}{\partial x_1^{k_1^j} \dots \partial x_d^{k_d^j}} (g \circ h).
\end{equation}
Following the explanations of \cite{hardy2006combinatorics}, and using the set $K^j := \{\underbrace{1, \dots 1}_{k_1^j}, \dots, \underbrace{d, \dots, d}_{k_d^j}\}$ we get
\begin{equation}
    \sum_{j=1}^{N_i} \frac{\partial^{k^j}}{\partial x_1^{k_1^j} \dots \partial x_d^{k_d^j}} (g \circ h) =
    \sum_{j=1}^{N_i}  \sum_{\sigma \in \partitioning(K^j)} \nu(\sigma) \partial^{|\sigma|} g(h) \prod_{s \in \sigma} \frac{\partial^{|s|} h}{\prod_{l \in s} \partial x_l} 
\end{equation}
Among $K^j$s partitions there are the trivial partition $\sigma_{trivial} = \{\{\underbrace{1, \dots 1}_{k_1^j}, \dots, \underbrace{d, \dots, d}_{k_d^j}\}\}$ and the atomic partition $\sigma_{atomic} = \{\underbrace{\{1\}, \dots, \{1\}}_{k_1^j}, \dots, \underbrace{\{d\}, \dots, \{d\}}_{k_d^j}\}$. The corresponding terms are
\begin{align}
    &\nu(\sigma_{atomic}) \partial^{k^j} g(h) \prod_{l \in K^j} \frac{\partial h}{\partial x_l}\\
    &\nu(\sigma_{trivial}) \partial g(h)\frac{\partial^{k^j} h}{\partial x_1^{k_1^j} \dots \partial x_d^{k_d^j}}.
\end{align}
The first term is $(D_i g) \circ h$. The latter is the term that corresponds to $D_i h$, which is the result of the previous propagation step in the setting forward mode. The other mixed terms include derivatives of lower degrees that are required to compute the derivatives of higher degree. These lower degree derivatives do not directly contribute to the differential operators. Therefore we can leverage the linearity trick here too: 
\begin{align}
    D_i(g \circ h) &= \nu(\sigma_{atomic}) (D_i g)(h) \prod_{l \in K^j} \frac{\partial h}{\partial x_l} + 
    \nu(\sigma_{trivial}) \partial g(h)D_i(h) \\
    &+ \sum_{i=1}^{N_i} \sum_{\underset{\sigma \neq \sigma_{atomic}}{\underset{\sigma \neq \sigma_{trivial}}{\sigma \in \partitioning(K^j)}}} \nu(\sigma) \partial^{|\sigma|} g(h) \prod_{s \in \sigma} \frac{\partial^{|s|} h}{\prod_{l \in s} \partial x_l}  .
\end{align}




%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
