We start with a self-contained introduction of Taylor-mode automatic differentiation~\citep[][Chapter 13]{griewank2008evaluating}.

\paragraph{Taylor mode automatic differentiation (scalar case).}
For simplicity, consider a scalar-valued function $f: \sR \to \sR, x \mapsto f(x)$, and a path $x(t): [-\tau, \tau] \to \sR$.
We will later introduce notation to seamlessly handle tensor-valued functions.
Assume we Taylor-expand the path in input space up to order $K$, i.e.\,$x(t) \approx \sum_{k=0}^K\nicefrac{t^k}{k!}
\left.\nicefrac{\partial^k x(t)}{\partial t^k}\right|_{t=0} \coloneqq \sum_{k=0}^K \nicefrac{t^k}{k!}
x_k$ with $x_k \coloneqq \left.\nicefrac{\partial^k f(x)}{\partial t^k}\right|_{t=0}$. This operation of creating a truncated Taylor approximation is called a \emph{$K$-jet}.
Our goal is to compute the $K$-jet of $f(x(t))$, given the $K$-jet of $x(t)$.
In notation, we want to compute $f_k \coloneqq \left.\nicefrac{\partial f(x(t))}{\partial t^k}\right|_{t=0}$ so that $f(x(t)) \approx \sum_{k=0}^K \nicefrac{t^k}{k!} f_k$:
\begin{center}
  \begin{tikzpicture}
    \node[align=center] (topleft) {Path in input space \\
      $x(t)$, $t \in [-\tau, \tau]$};

    \node[align=center, right=4cm of topleft] (topright) {Path in output space \\ $f(x(t))$};
    \draw [-Latex] (topleft.east) to node [midway, above] {$f$} (topright.west);

    \node[align=center, below=1cm of topright] (bottomright) {Truncated Taylor series
      \\
      $f(x(t)) \approx \sum_{k=0}^K \frac{t^k}{k!} f_k$
      \\
      {\color{maincolor}$(f_0, \dots, f_K)$}
    };
    \draw [-Latex] (topright.south) to node [midway, right] {$K$-jet} (bottomright.north);

    \node[align=center, below=1cm of topleft] (bottomleft) {Truncated Taylor series
      \\
      $x(t) \approx \sum_{k=0}^K \frac{t^k}{k!} x_k$
      \\
      {\color{maincolor} $(x_0, \dots, x_K)$}};
    \draw [-Latex] (topleft.south) to node [midway, left] {$K$-jet} (bottomleft.north);

    \draw [-Latex, maincolor] (bottomleft.east) to node [midway, above, maincolor] {Taylor mode} (bottomright.west);
  \end{tikzpicture}
\end{center}

Let's start by writing out the first two derivatives, for which we need the chain and product rules (for notational convenience, we omit the derivative evaluations at $t=0$):
\begin{subequations}\label{eq:taylor-mode-scalar}
  \begin{align}
    f_0
    &=
      f(x_0)
    \\
    f_1
    &=
      \frac{\partial f}{\partial x} \frac{\partial x}{\partial t}
      =
      \frac{\partial f}{\partial x} x_1
    \\
    f_2
    &=
      \frac{\partial^2 f}{\partial x^2} \left( \frac{\partial x}{\partial t} \right)^2
      +
      \frac{\partial f}{\partial x} \frac{\partial^2 x}{\partial t^2}
      =
      \frac{\partial^2 f}{\partial x^2} x_1^2
      +
      \frac{\partial f}{\partial x} x_2
    \\
    f_3
    &=
      \frac{\partial^3 f}{\partial x^3} \left( \frac{\partial x}{\partial t} \right)^3
      +
      3 \frac{\partial f^2}{\partial x^2} \frac{\partial x}{\partial t}
      \frac{\partial^2 x}{\partial t^2}
      +
      \frac{\partial f}{\partial x} \frac{\partial^3 x}{\partial t^3}
      =
      \frac{\partial^3 f}{\partial x^3} x_1^3
      +
      3 \frac{\partial^2 f}{\partial x^2} x_1 x_2
      +
      \frac{\partial f}{\partial x} \frac{\partial^3 x}{\partial t^3}
      \intertext{At this point, we can start to see a pattern.
      The incoming Taylor coefficients $\{x_k\}$ are processed into the outgoing Taylor coefficients $\{f_k\}$ using derivatives of $f$.
      And the coefficient $f_k$ use only incoming derivatives up to that order, $\{x_{k'<k}\}$.
      The number of terms grows with $k$, and we can see that the incoming Taylor coefficients are chosen from the \emph{integer partitioning of $k$}, for instance $\partitioning(3) = \{ \{1,1,1\}, \{1, 2\}, \{3\} \}$.
      Finally, there is a also a multiplicity term that depends on the set of from the integer partitioning.
      In total, this gives the well-known Fa\`a die Bruno formula~\cite{arbogast1800calcul,hardy2006combinatorics,faa1857note}
      }
      \label{eq:faa-di-bruno}
      f_k
    &=
      \sum_{\sigma \in \partitioning(k)}
      \nu(\sigma) \frac{\partial^{|\sigma|} f}{\partial x^{|\sigma|}}
      \prod_{s \in \sigma} x_s
      \quad
      \text{with multiplicity}
      \quad
      \nu(\sigma)
      =
      \frac{k!}{
      \left(\prod_{s \in \sigma} n_s!\right)
      \left(\prod_{s \in \sigma} s!\right)
      }
  \end{align}
\end{subequations}
where $n_s$ counts the occurrences of $s$ in an integer partitioning $\sigma$, for instance $n_1(\{1,1,3\}) = 2$.

\paragraph{The vector/matrix/tensor-valued case.}
So far, we assumed a scalar-to-scalar function.
To generalize of \cref{eq:faa-di-bruno} to higher dimensions, we will introduce the notation $\partial^kf[\vx_{i_1}, \dots, \vx_{i_k}]$ to indicate the contraction of the tensor containing the $k$-th order partial derivatives with the vectors $\vx_{i_1}, \dots \vx_{i_k}$.
With this notation, we can seamlessly write (with $\vx$ and $f(\vx)$ being vectors now)
\begin{align}
  \vf_k
  &=
    \sum_{\sigma \in \partitioning(k)} \nu(\sigma) \partial^{|\sigma|} f[\tensorprod{s \in \sigma} \vx_s]
\end{align}

\paragraph{Function composition.} Assume now that $f = g \circ h$. Then, Taylor mode proceeds as follows
\begin{align}\label{eq:taylor-mode-composition}
  \begin{pmatrix*}
    \vx_0
    \\
    \vx_1
    \\
    \vx_2
    \\
    \vdots
    \\
    \vx_K
  \end{pmatrix*}
  &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
    \begin{pmatrix*}[l]
      \vh_0 =  h(\vx_0)
      \\
      \vh_1 = \partial h[\vx_1]
      \\
      \vh_2 = \partial^2 h[\vx_1, \vx_1] + \partial h[\vx_2]
      \\
      \vdots
      \\
      \vh_K =
      \displaystyle\sum_{\mathclap{\sigma \in \partitioning(K)}} \nu(\sigma) \partial^{|\sigma|} h[\tensorprod{s \in \sigma} \vx_s]
    \end{pmatrix*}
    \overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
    \begin{pmatrix*}[l]
      \vg_0 =  g(\vh_0)
      \\
      \vg_1 = \partial g[\vh_1]
      \\
      \vg_2 = \partial^2 g[\vh_1, \vh_1] + \partial g[\vh_2]
      \\
      \vdots
      \\
      \vg_K =
      \displaystyle\sum_{\mathclap{\sigma \in \partitioning(K)}} \nu(\sigma) \partial^{|\sigma|} g[\otimes_{s \in \sigma} \vh_s]
    \end{pmatrix*}
    \overset{\text{(\ref{eq:taylor-mode-scalar})}}{=}
    \begin{pmatrix*}[l]
      \vf_0 =  f(\vx_0)
      \\
      \vf_1 = \partial f[\vx_1]
      \\
      \vf_2 = \partial^2 f[\vx_1, \vx_1] + \partial f[\vx_2]
      \\
      \vdots
      \\
      \vf_K =
      \displaystyle\sum_{\mathclap{\sigma \in \partitioning(K)}} \nu(\sigma) \partial^{|\sigma|} f[\tensorprod{s \in \sigma} \vx_s]
    \end{pmatrix*}
\end{align}
This equation tells us how to (i) propagate forward Taylor coefficients through a directed acyclic compute graph (DAG) that represents a chain of function compositions, and (ii) how the results relate to the composition's derivative \wrt the input space.

\subsection{Computing Laplacians}
Taylor mode from the previous sub-section serves as backbone for computing many differential operators like the Laplacian.
The fundamental problem is to figure out how to set the jet degrees $K$ and the Taylor coefficients $x_k$.

As an example, let's consider computing the Laplacian of a function $f: \sR^D \to \sR$,
\begin{align}
  \Delta f(\vx)
  =
  \sum_{d=1}^D \frac{\partial^2f(\vx)}{\partial x_d^2}
  =
  \sum_{d=1}^D \partial^2f[\ve_d, \ve_d]
\end{align}
Pattern matching with \cref{eq:taylor-mode-composition}, we see that one way to compute the Laplacian is to set $K=2$, $\vx_0 = \vx$, and then compute the forward-propagated coefficients of $\{(\vx_{1,d} = \ve_d, \vx_{2,d} = \vzero)\}_d$.
\begin{align}\label{eq:laplacian-naive}
  \begin{split}
    \begin{pmatrix*}
      \vx_0
      \\
      \{\vx_{1,d} \}
      \\
      \{\vx_{2,d} \}
    \end{pmatrix*}
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vh_0 =  h(\vx_0)
        \\
        \{\vh_{1,d}\} = \{\partial h[\vx_{1,d}]\}
        \\
        \{\vh_{2,d}\} = \{\partial^2 h[\vx_{1,d}, \vx_{1,d}] + \partial h[\vx_{2,d}]\}
      \end{pmatrix*}
    \\
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vg_0 =  g(\vh_0)
        \\
        \{\vg_{1,d}\} = \{\partial g[\vh_{1,d}]\}
        \\
        \{\vg_{2,d}\} = \{ \partial^2 g[\vh_{1,d}, \vh_{1,d}] + \partial g[\vh_{2,d}] \}
      \end{pmatrix*}
      \overset{\text{(\ref{eq:taylor-mode-scalar})}}{=}
      \begin{pmatrix*}[l]
        \vf_0 =  f(\vx_0)
        \\
        \{\vf_{1,d} \} = \{ \partial f[\vx_{1,d}] \}
        \\
        \{ \vf_{2,d} \} = \{ \partial^2 f[\vx_{1,d}, \vx_{1,d}] + \partial f[\vx_{2,d}] \}
      \end{pmatrix*}
    \\
    &\overset{\text{slice}}{\to} \{ \vg_{2,d} \}
    \\
    &\overset{\text{sum}}{\to} \sum_{d=1}^D \{ \vg_{2,d} \}
      \overset{\{(\vx_{1,d} = \ve_d, \vx_{2,d} = \vzero)\}_d}{=} \Delta f(\vx_0)
  \end{split}
\end{align}
In this scheme, we push forward $1 + D + D$ coefficients.
Can we do better?

\subsection{Using Linearity---Recovering the Forward Laplacian}
Looking at the scheme in \cref{eq:laplacian-naive}, we can realize that we can propagate the summation up the computation graph. To do that, we can use the linearity of $\partial g[\bullet]$.
Propagating the summation up the computation graph, we obtain (changes highlighted in \textcolor{maincolor}{color}):
\begin{align}\label{eq:laplacian-efficient}
  \begin{split}
    \begin{pmatrix*}
      \vx_0
      \\
      \{\vx_{1,d} \}
      \\
      \mathemph{\displaystyle\sum_{d=1}^D \vx_{2,d}}
    \end{pmatrix*}
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vh_0 =  h(\vx_0)
        \\
        \{\vh_{1,d}\} = \{\partial h[\vx_{1,d}]\}
        \\
        \mathemph{\displaystyle\sum_{d=1}^D \vh_{2,d}} = \displaystyle\sum_{d=1}^D \partial^2 h[\vx_{1,d}, \vx_{1,d}] + \partial h[\mathemph{\displaystyle\sum_{d=1}^D\vx_{2,d}}]
      \end{pmatrix*}
    \\
    &\overset{\text{(\ref{eq:taylor-mode-scalar})}}{\to}
      \begin{pmatrix*}[l]
        \vg_0 =  g(\vh_0)
        \\
        \{\vg_{1,d}\} = \{\partial g[\vh_{1,d}]\}
        \\
        \mathemph{\displaystyle\sum_{d=1}^D\{\vg_{2,d}\}} = \displaystyle\sum_{d=1}^D \partial^2 g[\vh_{1,d}, \vh_{1,d}] + \partial g[\mathemph{\displaystyle\sum_{d=1}^D\vh_{2,d}}]
      \end{pmatrix*}
      \overset{\text{(\ref{eq:taylor-mode-scalar})}}{=}
      \begin{pmatrix*}[l]
        \vf_0 =  f(\vx_0)
        \\
        \{\vf_{1,d} \} = \{ \partial f[\vx_{1,d}] \}
        \\
        \displaystyle\sum_{d=1}^D \vf_{2,d} = \displaystyle\sum_{d=1}^D \partial^2 f[\vx_{1,d}, \vx_{1,d}] + \partial f[\vx_{2,d}]
      \end{pmatrix*}
    \\
    &\overset{\text{slice}}{\to} \mathemph{\sum_{d=1}^D \{ \vg_{2,d} \}}
      \overset{\{(\vx_{1,d} = \ve_d, \vx_{2,d} = \vzero)\}_d}{=}
      \Delta f(\vx_0)
  \end{split}
\end{align}
In this scheme, the second-order coefficients are \emph{first summed, then propagated}, \ie we push forward $1 + D + 1$ vectors instead of $1 + D + D$.
In fact, this scheme in \cref{eq:laplacian-efficient} corresponds to the \emph{forward Laplacian} framework from \citet{li2023forward}.
However, we recovered it from a different starting point: by using Taylor mode in combination with sum propagations.

\paragraph{Weighted sums of second-order derivatives.}
We can generalize the scheme in \cref{eq:laplacian-efficient} to computing differential operators of the form $\sum_{i,j} C_{i,j} \nicefrac{\partial^2f}{\partial \evx_i \evx_j}$ with a symmetric coefficient matrix $\mC \in \sR^{D\times D}$. For simplicity, let's first assume that $\mC$ is positive semi-definite, which means we can express it as $\mC = \sum_{d=1}^{\rank(\mC)} \vv_d \vv_d^\top$ with real-valued $\vv_d \in \sR^D$, and therefore
\begin{align}\label{eq:weighted-laplacian}
  \sum_{i,j} C_{i,j} \frac{\partial^2f}{\partial x_i \partial x_j}
  =
  \sum_{d=1}^{\rank(\mC)} \partial^2f[\vv_d, \vv_d]\,.
\end{align}
This can be computed using \cref{eq:laplacian-naive}, and the simplified version in \cref{eq:laplacian-efficient}, by setting $\vx_{1,d} = \vv_d, \vx_{2,d} = \vzero$.
Also here, the second-order coefficients can be collapsed, and instead of pushing forward $1 + 2 \rank(\mC)$ vectors, we are left with $2 + \rank(\mC)$ vectors.
If $\mC$ is in-definite, then some $\vv_d$ will be purely complex, \ie have non-zero imaginary but zero real part, and some will be purely real.
We can either compute the differential operator in complex-valued arithmetic, or split the positive and negative parts of the spectrum into two separate collapsed jets.

\subsection{Using Linearity in Randomization Taylor Mode}
\citep{shi2024stochastic} introduce randomization into Taylor mode, similar to stochastic backpropagation~\cite{oktay2021randomized}.
This lowers the computational cost when $D$ is prohibitively large for exact evaluation of differential operators.
Take the weighted Laplacian from \Cref{eq:weighted-laplacian} and assume we have access to random vectors $\rvv$ with and $\E[\vv \vv^{\top}] = \mC$.
Then we can draw $S \ll D$ random vectors $\vv_1, \vv_2, \dots, \vv_S \overset{\text{i.i.d.}}{\sim} \rvv$ and compute
\begin{align}
  \begin{split}
    \E[\partial^2f[\rvv, \rvv]]
    &=
      \E \left[
      \rvv^{\top} \frac{\partial^2 f}{\partial \vx \partial \vx} \rvv
      \right]
      =
      % \E \left[
      %   \sum_{i,j} \ervv_i \frac{\partial^2 f}{\partial x_i \partial x_j} \ervv_j
      % \right]
      % =
      \sum_{i,j} \frac{\partial^2 f}{\partial x_i \partial x_j}
      \E \left[
      \ervv_i  \ervv_j
      \right]
      =
      \sum_{i,j} \frac{\partial^2 f}{\partial x_i \partial x_j} C_{i,j}
    \\
    &\approx
      \frac{1}{S}
      \sum_{s=1}^S
      \partial^2 f[\vv_s, \vv_s]\,.
  \end{split}
\end{align}
Again, we see that the second-order coefficients can be collapsed when setting $\vx_{1,d} = \vv_d, \vx_{2,d} = \vzero$ in \cref{eq:laplacian-naive,eq:laplacian-efficient}.

\subsection{Using Linearity in Higher-Order Operators}
%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
