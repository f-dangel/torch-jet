Here, we describe how we implemented the collapsing process of Taylor mode and empirically validate the performance improvements of collapsing standard Taylor mode on the previously described operators.

\paragraph{Design decisions \& limitations.}
JAX~\cite{bradbury2018jax} already offers an--albeit experimental---Taylor mode implementation~\cite{bettencourt2019taylor}.
However, we found it challenging to capture the computation graph and modify it using JAX's public interface.
In contrast, PyTorch \cite{paszke2019pytorch} provides \texttt{torch.fx} \cite{reed2022torch}, which offers a user-friendly interface to capture and transform computation graphs purely in Python.
Hence, we re-implemented Taylor mode in PyTorch, taking heavy inspiration from the JAX implementation.

This deliberate choice imposes certain limitations.
First, as of now, our Taylor mode in PyTorch supports only a small number of primitives, because the Taylor arithmetic in \cref{eq:faa-di-bruno} needs to be implemented case by case (this of course also applies to JAX's Taylor mode, which has broader operator coverage).
Second, functions represented by \texttt{torch.fx} graphs can, at the moment, not be accelerated with \texttt{torch.compile}.
In our PyTorch experiments, we therefore report un-compiled implementations. 
Our JAX experiments with the Laplacian confirm that relative performance remains consistent when using \texttt{jit} (\cref{sec:jax-benchmark}). 
Last, while our Taylor mode implementation is competitive with JAX's, we did not fully optimize it (\eg we do \emph{not} use in-place operations, and we do \emph{not} implement the efficient scheme from \citet[][\S13]{griewank_evaluating_1999}, but stick to \cref{eq:faa-di-bruno}).
Given our implementation's superiority compared to nested first-order AD that we demonstrate below, these are promising future efforts that will further improve performance, and we believe that making Taylor mode available to the PyTorch community is also an important step towards establishing its use.

\paragraph{Usage.}
Our implementation takes a PyTorch function (\eg a neural net) and first captures its computational graph using \texttt{torch.fx}'s symbolic tracing mechanism.
Then, it replaces each operation with its Taylor arithmetic, which yields the computational graph of the function's $K$-jet.
Users can use this vanilla Taylor mode to define their differential operator's computation.
The collapsing is done by again by a function \texttt{simplify}, which again traces the computation, then rewrites the graph, propagating the summation of highest coefficients up to the leafs.
This requires one backward traversal of the graph.
The resulting graph performs the same computation, but propagates summed coefficients, \ie uses collapsed Taylor mode.
See \cref{fig:interface-overview} for a visual walk-through of the procedure.

\input{figures/torch_benchmark}
\input{tables/torch_benchmark}

\paragraph{Experimental setup.}
We empirically validate our proposed collapsing approach in PyTorch.
We compare collapsed Taylor mode with vanilla Taylor mode and nested first-order AD on the previously discussed differential operators using an Nvidia RTX 6000 with 24 GiB memory.
To implement the (weighted) Laplacian and its stochastic counterpart, we use vector-Hessian-vector products (VHVPs) in forward-over-reverse order, as recommended \cite{griewank2008evaluating,dagreou2024how}.
For the biharmonic operator, we simply nest two VHVPs.
We use a $D \to 768 \to 768 \to 512 \to 512 \to 1$ MLP $f_\vtheta$ with tanh activations and trainable parameters $\vtheta$, and compute the differential operators on multiple data in parallel, \ie on a batch of size $N$.

For all implementations, we measure three performance metrics (memory allocation does not fluctuate, hence we determine it in a single run):
\begin{enumerate}[leftmargin=0.5cm]
    \item \textbf{Runtime.} We report the smallest value of 50 independent repetitions.
    
    \item \textbf{Peak memory (non-differentiable \wrt $\vtheta$)} when computing the PDE operator inside a \texttt{torch.no\_grad} context, \ie we are only interested in its value (\eg in VMC \cite{pfau2020ab}).
    
    \item \textbf{Peak memory (differentiable \wrt $\vtheta$)} when computing the PDE operator inside a \texttt{torch.enable\_grad} context. This covers applications like PINNs or alternative VMC works \cite{webber2022rayleigh, toulouse2007optimization} that demand the PDE operator be differentiable \wrt the neural network's parameters $\vtheta$.
\end{enumerate}

\paragraph{Results.}
\Cref{fig:benchmark} visualizes the computational demands' growth \wrt the batch size (exact) and number of Monte-Carlo samples (stochastic) for fixed dimensions $D$.
We used a 5-layer MLP with $\tanh$ activation and around 3M parameters, similar to the ones employed in other works on PINNs \cite{dangel2024kroneckerfactored,shi2024stochastic}.
As the scalings with batch size and Monte-Carlo samples are both linear, we quantify the results by fitting linear functions and reporting their slopes (\ie, time and memory added per datum/sample) in \Cref{tab:benchmark}. We make the following observations:
\begin{itemize}[leftmargin=0.5cm]
\item \textbf{Collapsed Taylor mode accelerates standard Taylor mode.}
  When comparing collapsed with standard Taylor mode, we observe that the measured runtime differences correspond well with the theoretical prediction.
  \Eg, for the Laplacian, the ratio of propagated vectors between standard and collapsed is $\nicefrac{(2 + D)}{(1 + 2D)} \approx 0.5$; we observe that adding one datum adds $0.60ms$ to standard and $0.33ms$ to collapsed Taylor mode.
  Similar for the weighted Laplacian and for their stochastic versions.
  This means that we can compute these operators with twice the batch size or MC-samples in the same time as had we used standard Taylor mode.

\item \textbf{Collapsed Taylor mode consistently outperforms nested 1\textsuperscript{st}-order AD.}
  For the exact and stochastic Laplacian and weighted Laplacian, collapsed Taylor mode is roughly twice as fast as nested AD while using 70-80\% of the latter's memory.
  The only exception is for the differentiable Biharmonic, which consumes about 20\% more memory, but uses only 66\% time.
  For the randomized Biharmonic, we observe even greater runtime and memory improvements by roughly 9 and 3 (or 6 if non-differentiable)x.

  \todo{FD: I think one reason why we get bigger improvements for the stochastic Bi-Laplacian is that I have to use \texttt{vmap} twice and this might mess up performance (hard to verify though).
  }

\item \textbf{If the PDE operator can be non-differentiable, both standard and Taylor mode require significantly less memory than nested 1\textsuperscript{st}-order AD.}
  This aligns with our understanding of Taylor mode, which performs a single forward propagation and does not need to store any intermediates for future differentiation, in contrast to the nested differentiation.
\end{itemize}

Because of its special structure, there is another efficient scheme for computing the Biharmonic Operator that nests twice the Laplacian.
In the appendix, we briefly discuss this approach in combination with our collapsed Taylor mode.
However, our focus here was to show that our idea applies to other differential operators, therefore making it a relevant contribution.


We also conducted experiments with JAX (+ \texttt{jit}) to rule out artifacts from the ML framework.
In general, we find that the choice of ML framework does not affect the results.
\Eg, for computing the Laplacian with nesting, we found that PyTorch increases by 0.61ms and 4.4 and 2.2 MiB per datum, while JAX increases by 0.57ms and 6.0, and 1.4 MiB per datum (\Cref{fig:vanilla-taylor-not-enough,tab:jax-benchmark}).
We find the same trend when comparing our collapsed Taylor mode and JAX's forward Laplacian.
Interestingly, we noticed that JAX's Taylor mode was consistently slower than our PyTorch implementation, despite using \texttt{jit}.
We conclude from these results that (both ours, as well as the existing JAX) Taylor mode implementations still have potential for improvements that may further improve the margin to nested first-order.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
