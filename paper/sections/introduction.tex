We have to justify how our approach goes beyond the forward Laplacian.
One way is that we can compute a wider family of second-order derivatives (weighted sums).
Another one is that our approach is compatible with techniques to scale to extremely high dimensions (randomization).

Our contributions are the following:
\begin{enumerate}
    \item We describe `collapsed' Taylor mode which allows to collapse the highest Taylor coefficients during the forward propagation whenever we compute a weighted sum of said derivatives.
    This trick has first been used in the `forward Laplacian' framework, which however does not rely on Taylor mode.
    Our collapsed Taylor mode improves over it in two ways.
    First, it separates the trick into using standard Taylor mode, combined with simple graph rewrites based on linearity.
    Second, it generalizes to other differential operators, and can also be applied to recently proposed randomization techniques for Taylor mode.
    
    \item This leads to a conceptually clean separation of concepts, allowing for a simple implementation that can handle all scenarios and does not require manual implementations for each differential operator.
    To demonstrate this, we build a Taylor mode library for PyTorch and equip it with the necessary graph simplifications using the \texttt{torch.fx} library, and show how collapsed Taylor mode accelerates the computation of various differential operators.

    \item Ideally, we find specially structured differential operators for which our trick is especially suited, and classify them.
    One example is the trace of derivative tensors of arbitrary order, although this seems to be an operator that is not practically relevant.
\end{enumerate}
\newpage

\begin{figure}[t]
  \centering
\includegraphics*{../jet/exp/exp01_benchmark_laplacian/figures/architecture_tanh_mlp_768_768_512_512_1_device_cuda_dim_50_name_dangel2024kroneckerfactored_vary_batch_size.pdf}
\caption{ \textbf{Computing the Laplacian with collapsed Taylor mode is faster \emph{and} uses less memory than nested first-order AD; naive Taylor mode is competitive in time but uses more memory.}
  We compare different strategies for computing Laplacians for a $50\to 768 \to 768 \to 512 \to 512 \to 1$ MLP with tanh activations for varying batch sizes on an NVIDIA A40 GPU.
  Solid markers correspond to computing differentiable Laplacians (\eg used in PINNs), opaque markers compute a non-differentiable Laplacian (\eg used in VMC).
  Averaged over all batch sizes, \emph{our collapsed Taylor mode uses only 80\% memory of nested first-order AD (70\% in the non-differentiable setting), and 50\% of time}.
  In contrast, naive Taylor mode uses 120\% memory of nested first-order AD (70\% in the non-differentiable setting) and 95\% of time.
}
  \label{fig:benchmark-laplacians}
\end{figure}

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
