Taylor-mode AD allows the computation of higher-order
derivatives—as needed, e.g., for PDE operators—through a suitable combinations of Taylor coefficients. The latter can easily be propagated using the chain rule. To illustrate Taylor-mode AD, consider the one-dimensional case $f: \mathbb{R} \to \mathbb{R}$. When the input variable $x$ is interpreted as a univariate Taylor polynomial
of degree $K$, i.e., $x(t) = \sum_{k=0}^K \frac{t^k}{k!} x_k$ and $f$ is smooth enough, one can also evaluate higher order derivatives of the path $f(x(t))$ that yield a corresponding Taylor polynomial of $f$, i.e., $\sum_{k=0}^K \frac{t^k}{k!} f_k$  with $f_k := \left. \frac{d^k}{dt^k} f(x(t)) \right|_{t=0}$. Using the chain rule, one obtains for the simple example considered here and $K=3$ the coefficients
\input{figures/taylor_signature}
\begin{align}
\label{eq:taylor-mode-scalar}
\begin{split}
    f_0
    &=
    f(x_0)
    \\
    f_1
    &=
    \partial f(x_0) x_1
    \\
    f_2
    &=
    \partial^2 f(x_0)x_1^2
    +
    \partial f(x_0) x_2
    \\
    f_3
    &=
    \partial^3 f(x_0)x_1^3
    +
    3 \partial^2 f(x_0) x_1 x_2
    +
    \partial f(x_0) x_3.
    \end{split}
\end{align}
 The general formula for $f_k$ was originally given by Faà di Bruno already in 1856 and has since been extended to the multivariate case by Fraenkel in 1978, see, e.g., \cite{arbogast1800calcul,hardy2006combinatorics,faa1857note}. These formulas  serve as foundation for Taylor-mode AD to compute derivative of higher order, see, e.g., \citep[][Chapter 13]{griewank2008evaluating}. Choosing in our example, $x_1 = 1, x_2 = x_3 = 0$ results in $f_1 = \partial f(x_0), f_2 = \partial^2 f(x_0)$ and $f_3 = \partial^3 f(x_0)$. To exploit these formula in a more general case, e.g., the calculation of differential operators,  leveraging an input Taylor polynomial, we now introduce some notation.

 First, one has to note that the univariate Taylor polynomials of degree $K$ are frequently called $K$-jets in the machine learning community. Therefore, we will use this notation from now on. Furthermore, we denote vectors, matrices, and tensors using boldface. Given $K$ vectors $\vv_1, \dots, \vv_K \in \mathbb{R}^D$, their tensor product is written as
\begin{equation}
    \otimes_{k=1}^K \vv_k = \vv_1 \otimes \ldots \otimes \vv_K
    \in \left( \mathbb{R}^D \right)^{\otimes K},
\end{equation}
with components given by
\begin{equation}
     \left(\otimes_{k=1}^K \vv_k\right)_{d_1, \dots, d_K}
    = (\vv_1)_{d_1} \cdot \ldots \cdot (\vv_K)_{d_K}
    \quad \text{for } d_1, \dots, d_K \in \{1, \dots, D\}.
\end{equation}
To simplify the notation we use $(\vv)^{\otimes K} = \otimes_{k=1}^K \vv$ and frequently also the inner product
\begin{align}\label{eq:derivative-tensor-scalar-product}
    \left\langle
    \tA, \tB
    \right\rangle
    \coloneqq
    \sum_{d_1}
    \sum_{d_2}
    \dots
    \sum_{d_K}
    \tA_{d_1, d_2, \dots, d_K}
    \tB_{d_1, d_2, \dots, d_K},
\end{align}
where $\tA, \tB \in \left(\mathbb{R}^{D}\right)^{\otimes K}$. The $k$-th derivative of a function $\vf: \mathbb{R}^D \to \mathbb{R}^m$ is denoted by $\partial^k \vf$.

Also in the more general vector-valued case, the evaluation of $\vf$'s $K$-jet at an argument $\vx_0 \in \mathbb{R}^n$ starts with the extension of $\vx_0$ to a smooth path $\vx: \mathbb{R} \to \mathbb{R}^D$ such that $\vx(0) = \vx_0$. Then, the $K$-jet of $\vf$ is defined as $J^K \vf : \mathbb{R} \to \mathbb{R}^m$, $(J^K \vf)(t) := \sum_{k=0}^K \frac{t^k}{k!} \vf_k$ with $\vf_k := \left. \frac{d^k}{dt^k} \vf(\vx(t)) \right|_{t=0} $, where the evaluation of $\vf_k$ requires the $K$-jet $(J^K \vx)(t) := \sum_{k=0}^K \frac{t^k}{k!} \vx_k$ as can be seen in the simple example \eqref{eq:taylor-mode-scalar} above and illustrated in Figure~\ref{fig:utp}.

Next, we have to consider the calculation of the derivative values $\vf_k$. As is common for AD, the computation of $\vf_k$
known as Taylor-mode AD relies on a decomposition of $\vf$ into elemental functions for which the derivative rules are known and the application of the chain rule. In the simplest case, $\vf$ might be given as the decomposition $\vf = \vg \circ \vh$ of two elemental functions $\vg$ and $\vh$. Having defined $J^K\vx$, Taylor-mode AD calculates the $K$-jet of $\vf$ by successively evaluating all $\vf_k$ through the chain rule using Faà di Bruno's formula mentioned above. For $\vf = \vg \circ \vh$ using the notation introduced above, this formula reads
\begin{align}
\label{eq:faa-di-bruno}
  \vf_k = \vg_k
  =
    \sum_{\sigma \in \partitioning(k)}
    \nu(\sigma)
    \left<
    \partial^{|\sigma|} \vg,
    \tensorprod{s \in \sigma} \vh_s
    \right>,
\end{align}
with $\partitioning(k)$ being the integer partitioning of $k$ and multiplicity
\begin{equation}
    \nu(\sigma)
    =
    \frac{k!}{
    \left(
    \prod_{s \in \sigma
    }
    n_s!
    \right)
    \left(
    \prod_{s \in \sigma}
    s!
    \right)
     }.
\end{equation}
Here, $n_s$ counts occurrences of $s$ in an integer partitioning $\sigma$, e.g., $n_1(\{1,1,3\}) = 2$ and $n_3 = 1$. There are explicit formulas for common elemental functions that yield the derivative for given $K$-jets of the inputs with a quadratic complexity in the degree $K$, see~\citep[][Chapter 13]{griewank2008evaluating}.

Our contribution modifies Taylor-mode AD to incorporate structure of common differential operators.
To explain our approach systematically, we will use the following Taylor-mode AD scheme, which derives the $K$-jet for a decomposed $\vf = \vg \circ \vh$ given $J^K\vx$:
\begin{align}\label{eq:taylor-mode-composition}
\begin{split}
      &\begin{pmatrix*}
        \vx_0
        \\
        \vx_1
        \\
        \vx_2
        \\
        \vdots
        \\
        \vx_K
      \end{pmatrix*}
      \!\overset{\text{(\ref{eq:faa-di-bruno})}}{\to}\!
        \begin{pmatrix*}[l]
          \vh_0 \!=\!  \vh(\vx_0)
          \\
          \vh_1 \!=\!  \left<
          \partial \vh(\vx_0),
          \vx_1
          \right>
          \\
          \vh_2 \!=\! \left<
          \partial^2 \vh(\vx_0),
          \vx_1 \otimes \vx_1
          \right>
          \!+\!
          \left <
          \partial \vh(\vx_0),
          \vx_2
          \right>
          \\
          \vdots
          \\
          \vh_K \!=\!
          \displaystyle \sum_{
          \mathclap{
          \sigma \in \partitioning(K)
          }
          }
          \nu(\sigma) \left<
          \partial^{|\sigma|} \vh(\vx_0),
          \tensorprod{s \in \sigma} \vx_s
          \right>\!\!\!
        \end{pmatrix*}
        \\
        &\!\!\overset{\text{(\ref{eq:faa-di-bruno})}}{\to}\!
         \left(\!\!\!
          \begin{array}{l}
          \vg_0 \!=\!  \vg(\vh_0)
          \\
          \vg_1 \!=\! \left<
          \partial \vg(\vh_0),
          \vh_1
          \right>
          \\
          \vg_2 \!=\! \left<
          \partial^2 \vg(\vh_0),
          \vh_1 \!\otimes\! \vh_1\right>
          \!+\!
          \left< \partial \vg(\vh_0),
          \vh_2
          \right>
          \\
          \vdots
          \\
          \vg_K \!=\!
          \displaystyle\sum_{
          \mathclap{
          \sigma \in \partitioning(K)
          }
          }
          \nu(\sigma) \left<
          \partial^{|\sigma|} \vg(\vh_0),
          \tensorprod{s \in \sigma} \vh_s
          \right>
          \end{array}
          \!\!\!\!
        \right)
        \!\!=\!\!
        \left(\!\!\!
          \begin{array}{l}
          \vf_0 \!=\!  \vf(\vx_0)
          \\
          \vf_1 \!=\! \left<
          \partial \vf(\vx_0),
          \vx_1
          \right>
          \\
          \vf_2 \!=\! \left<
          \partial^2 \vf(\vx_0),
          \vx_1 \!\otimes\! \vx_1
          \right>
          \!+\!
          \left< \partial \vf(\vx_0),
          \vx_2
          \right>
          \\
          \vdots
          \\
          \vf_K \!=\!
          \displaystyle\sum_{
          \mathclap{
          \sigma \in \partitioning(K)
          }
          }
          \nu(\sigma) \left<
          \partial^{|\sigma|} \vf(\vx_0),
          \tensorprod{s \in \sigma} \vx_s
          \right>
          \end{array}
          \!\!\!\!
          \right)
    \end{split}
\end{align}
%This procedure explains how to compute the coefficients that occur in the $K$-jets of nodes in the computational graph of $\vf$'s decomposition, and how the result relates to the input $K$-jet.


As can be seen for all $K$-jets, the $K$-th coefficient is computed as the sum of weighted inner products over all derivative degrees $|\sigma|$ and the tensor product of corresponding input coefficients. This tensor product is highly nonlinear in general. However, in the following section, we propose a new approach that takes advantage of a linear structure in the input coefficients to be propagated through the computational graph. This linear structure is often exposed by differential operators
that may include also mixed-partial derivatives. Being part of a higher-order derivative tensor, they are not directly computable using a single $K$-jet. Therefore, as detailed below we leverage the work of Griewank et al. \cite{griewank_evaluating_1999} to compute the whole $K$-th derivative tensor with a minimal number of $K$-jets propagated through the computational graph by choosing the coefficients of the $K$-jets appropriately.


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
