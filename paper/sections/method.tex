We exploit the linearity in the $K$th coefficients when propagating a family of $K$-jets through a computational graph.
Such families of $K$-jets were already considered for the Forward-Laplace \cite{li2023forward}, Randomized Taylor-Mode ~\cite{shi2024stochastic, hu2023hutchinson} and the generalized trace operator\todo{T: Cite the papers that also considered this, and maybe an application?}.
Then, we show that differential operators with a suitable structure can be computed via the propagation of an appropriately chosen family of jets using an algorithm of Griewank et al.
\cite{griewank_evaluating_1999}.
The result will also include the approach of \cite{shi2024stochastic} for computing arbitrary differential operators.
However, our approach only needs jets of the highest derivative degree occurring in the differential operator.
It has to be emphasized that our method can be completely automated.
Hence, it can be implemented as part of a JIT-toolchain.
\todo{T: BTW the automation is currently just a claim of me.
  I did not fully think through the idea of automating the TTC formula.
}

\subsection{Exploiting Linearity to Collapse Taylor-mode AD}
To derive the proposed method, the considerations start with a sum over $K$th-order directional derivatives of a function $\vf$ like the Laplacian.
In our notation, this sum can be represented as
\begin{align}\label{eq:sum-k-directional}
  \textstyle % Comment out this line if we have enough space
  \sum_{n=1}^N\left<
  \partial^{K} f(\vx_0),
  \otimes_{k=1}^K \vv_n
  \right>.
\end{align}
For the Laplacian, one has $K = 2$, $N$ equals the dimension of $\vx$, i.e., $N= D$, and the vectors are chosen as the unit vectors $\vv_n = \ve_n$.
Instead of nesting derivative calculations to compute the full $K$-th order derivative tensor, $K$-jets can be utilized to efficiently calculate only the required derivatives.
To this end, one could propagate a family of $K$-jets through the whole computational graph using Taylor-mode AD as illustrated in the appendix for a simple example in \cref{eq:sum-taylor-mode-naive}.
Note, that it is crucial to select appropriate input coefficients $\vx_i$.
To compute \cref{eq:sum-k-directional}, one has to use as $n$-th $K$-jet $\vx_{0, n} = \vv_n$ and $\vx_{k, n} = \vzero\in\R^D$ as input for the Taylor-mode AD.

Without further optimizations, the standard Taylor-mode AD propagates $1 + KN$ vectors through every node of the computational graph, where the length of the vectors depend on the individual node.
Subsequently, one has to sum up the corresponding highest derivatives to obtain \cref{eq:sum-k-directional}.
This is illustrated in the appendix, see \cref{eq:sum-taylor-mode-naive}, for our simple example.
The approach we propose here is based on the observation that there is a special element in the set of integer partitions $\partitioning(K)$, namely the trivial partition $\tilde{\sigma}= \{K\}$ corresponding to the term $ \nu(\tilde{\sigma}) \left< \partial \vg(\vh_0), \vh_{K, n} \right>$ of the Fa√† di Bruno formula with $\nu(\tilde{\sigma}) = 1$.
\begin{equation}\label{eq:faa-di-bruno-expanded}
  \textstyle % Comment out this line if we have enough space
  \vf_{K, n}
  =
  \vg_{K,n}
  =
  \sum_{
    \sigma \in \partitioning(K) \setminus \{\tilde{\sigma}\}
  }
  \nu(\sigma) \left<
    \partial^{|\sigma|} \vg(\vh_0),
    \tensorprod{s \in \sigma} \vh_{K, n}
  \right>
  +
  \left<
    \partial \vg(\vh_0),
    \vh_{K, n}
  \right>
\end{equation}
for our simple example.
The last inner-product term on the right-hand side is linear in $\vh_{K,n}$, marking the key property for our method.
To compute the sum of $\{\vg_{K, n}\}_n$, i.e., \cref{eq:sum-k-directional}, it is possible to directly propagate their sum, like $\sum_n \vh_{K,n}$, due to the linearity of $\left<\partial \vh, \bullet \right>$ and $\left< \partial \vg, \bullet \right>$ instead of passing the highest coefficients, like $\{\vh_{K,n}\}_n$, separately along the nodes of the computational graph.
This observation also holds for general functions $ \vf$.
Combining this insight with Taylor-mode of AD yields our proposed {\em collapsed Taylor-mode AD}.
It propagates only $1 \!+\!
(K \!- \!1)N \!+\!
1$ vectors at every node in the computational graph, where the length of the vectors varies for each individual node in the same way as for standard Taylor-mode AD.
For our simple example, the collapsed Taylor-mode AD is detailed in the appendix, see \cref{eq:sum-taylor-mode-efficient}, where the changes are highlighted in \textcolor{\colorcTM}{\colorcTMname}.
To emphasize that the saving of $N-1$ coefficients at every node in the computational graph is indeed significant for common applications, important operators are discussed below.

\subsection{Second-order Operators and Traces}
A highlight of our general approach, i.e., the collapsed Taylor-mode AD for \cref{eq:sum-k-directional}, is that it naturally includes common important differential operators or their approximation.

\paragraph{Laplace Operator} The Laplacian plays a central role in many physical and engineering applications, including electrostatics, fluid dynamics, and heat conduction.
In the context of variational Monte Carlo simulations in quantum mechanics \cite{foulkes2001quantum, pfau2020ab}, the Laplacian of the wave function must be evaluated repeatedly as part of the wave function optimization process.
For a function $f: \sR^D \to \sR$, the Laplacian is given as
\begin{align}\label{eq:laplacian}
  \textstyle % Comment out this line if we have enough space
  \Delta f(\vx_0)
  :=
  \sum_{d=1}^D \frac{\partial^2 f(\vx_0)}{\partial x_d^2}
  =
  \begin{cases}
  = \sum_{d=1}^D \left<
  \partial^2 f(\vx_0),
  \ve_d \otimes \ve_d
  \right>. \text{(exact Taylor mode)}
  \\
  \approx ... \text{(stochastic Taylor mode)}
  \end{cases}
\end{align}
\todo{Write down the stochastic cases for the Laplacian, weighted Laplacian, and Biharmonic.}
If $f$ can be decomposed into $g \circ \vh$, where $\vh$ is vector-valued, as is the case if $\vh$ is a hidden layer of a neural network, then selecting $K = 2$, $N=D$ and $\vv_n = \ve_n$ lifts \cref{eq:laplacian} into the setting of \cref{eq:sum-k-directional}.
For standard Taylor-mode AD, one propagates $1 + 2D$ vectors through the computational graph, see \cref{eq:laplacian-naive}.
If we apply the collapsed Taylor-mode as proposed here, $1 + D + 1$ vectors are propagated.
The corresponding propagation of $K$-jets for the Laplacian is detailed in \cref{eq:laplacian-efficient}, where changes are highlighted again in \textcolor{\colorcTM}{\colorcTMname}.
Note that this scheme agrees with the one presented in \cite{li2023forward}, but here it is embedded into a general framework.



\paragraph{Weighted Laplacian.} %\todo{We can make the connection \cite{hu2023hutchinson} equation (5) clear and discuss the case of $\mC$ being a function of the input}

A natural generalization of the Laplace operator involves forming weighted sums of second derivatives.
Given a positive semi-definite matrix $\boldsymbol{C}\in\mathbb R^{D\times D}$, we consider the operator
\begin{align}\label{eq:weighted-laplacian}
  \textstyle % Comment out this line if we have enough space
  \sum_{i,j=1}^D \boldsymbol{C}_{i,j} \frac{\partial^2 \vf(\vx_0)}{\partial x_i \partial x_j}.
\end{align}
This expression represents, for example, the diffusion term in a Kolmogorov-type partial differential equation \cite{hu2023hutchinson}.
It is often equivalently written as $\Tr ( \msigma \msigma^\top \partial^2 \vf(\vx_0))$, where $\msigma$ may be a function $\msigma:\mathbb R^{D}\to \mathbb R^{D\times D}$.
Since $\boldsymbol{C}$ is positive semi-definite it can be expressed as $\mC = \sum_{n=1}^{\rank(\mC)} \vc_n \vc_n^\top$ for suitable $\vc_n \in \sR^D$ matching \cref{eq:sum-k-directional} with $K = 2, N = \rank(\mC)$ and $\vv_n = \vc_n$.
This requires the propagation of $1 + \rank(\mC) + 1$ vectors instead of $1 + 2\rank(\mC)$.
An important special case of \cref{eq:weighted-laplacian} is the Hutchinson Trace Estimation for the Laplacian \cite{hu2023hutchinson}, which is also computable like the Forward-Laplacian scheme \cite{li2023forward}, leveraging the collapsed Taylor-mode.
For our simple example $\vf = \vg \circ \vh$ from above, the differential operator \eqref{eq:weighted-laplacian} can be calculated using \cref{eq:sum-taylor-mode-efficient}.

% Another important differential operator is represented by a symmetric positive semi-definite (PSD) matrix $\boldsymbol{C} \in \left(\mathbb{R}^D\right)^{\otimes 2}$:
% \begin{align}\label{eq:weighted-laplacian}
%   \sum_{i,j} \boldsymbol{C}_{i,j} \frac{\partial^2 \vf(\vx_0)}{\partial x_i \partial x_j}.
% \end{align}
% Those operators occur, for example, as
% \begin{equation*}
%   \Tr (\msigma \msigma^\top \partial^2 \vf(\vx_0)
% \end{equation*}
% in Fokker-Planck and Hamiltonian-Jacobi Bellmann equations. \todo{need citations} Since $\boldsymbol{C}$ is PSD it can be expressed as $\mC = \sum_{n=1}^{\rank(\mC)} \vc_n \vc_n^\top$ for suitable $\vc_n \in \sR^D$. Therefore, it fits the setting of \cref{eq:sum-k-directional} with $K = 2, N = \rank(\mC)$ and $\vv_n = \vc_n$, and can be compute using \cref{eq:sum-taylor-mode-efficient}. This requires the propagation of $1 + \rank(\mC) + 1$ vectors instead of $1 + 2\rank(\mC)$.


\paragraph{Traces of higher-order derivative tensors}
\todo{We probably have to remove this to save space.}
The higher-order Laplacian, e.g., considered as theoretical example in \cite{shi2024stochastic}, or as application in ...\todo{add references}  is also naturally expressible in our framework as
\begin{align}\label{eq:trace-differential-operator}
  \textstyle % Comment out this line if we have enough space
  \Tr( \partial^K f(\vx) )
  \coloneqq
  \sum_{d=1}^D
  \frac{\partial^K f(\vx_0)}{\partial \evx_d^K}
  =
  \sum_{d=1}^D
  \left<
  \partial^K f(\vx_0),
  \otimes_{i=1}^K \ve_d
  \right>,
\end{align}
with $N = D$ and $\vv_n = \ve_n$.

\subsection{Collapsed Taylor-mode AD for arbitrary mixed-partial derivatives}
Despite being already general enough to unify various approaches for the computation of important differential operators, up to now our framework cannot handle operators that include arbitrary mixed-partial derivatives, i.e.
operators where we have to sum over different directions in every inner-product.
For orders higher than two, such operators can not be decomposed into the weighted sums scheme given in \cref{eq:weighted-laplacian}.
To tackle this problem, we extend our framework to transform a differential operator containing mixed-partial derivatives into a family of jets using the result of Griewank et al.~\cite{griewank_evaluating_1999}.

As one important example, we consider the Biharmonic Operator, i.e.,
\begin{align}
  \label{eq:biharm}
  \textstyle % Comment out this line if we have enough space
  \Delta^2 \vf(\vx_0)
  \coloneqq
  \sum_{n_1=1}^D \sum_{n_2=1}^D
  \frac{\partial^4 \vf(\vx_0)}{\partial \evx_{n_1}^2 \partial \evx_{n_2}^2}
  =
  \sum_{n_1=1}^D \sum_{n_2=1}^D
  \left<
  \partial^4 \vf(\vx_0),
  \ve_{n_1}^{\otimes 2} \otimes \ve_{n_2}^{\otimes 2}
  \right>.
\end{align}
Once more, we introduce a notation that allows us to cover also more general cases.
For the maximal derivative degree $K$, $P \leq K$ different sums and arbitrary directions $\vv_{n_k}$, we can rewrite \cref{eq:biharm} as
\begin{align}\label{eq:sums-k-directional}
  \textstyle % Comment out this line if we have enough space
  \sum_{n_1=1}^{N_1} \dots \sum_{n_P=1}^{N_P}\left<
  \partial^{K} f(\vx_0),
  \vv_{n_1}^{\otimes p_1}
  \otimes \ldots \otimes
  \vv_{n_P}^{\otimes p_P}
  \right>,
\end{align}
with $K = 4, P = 2, N_1 = N_2 = D, \vp = (2, 2), \vv_{n_1} = \ve_{n_1}$ and $\vv_{n_2} = \ve_{n_2}$.
Note that for $\vp = (p_1, \dots, p_P)$ one must have $\sum_i p_i = K$.
Eq.~\eqref{eq:sums-k-directional} may include summands where not all directions coincide as it is the case for \cref{eq:sum-k-directional}.
Those summands are not directly representable by $K$-jets and thus, not computable via collapsed Taylor-mode AD.
However, Griewank et al.
derived in \cite{griewank_evaluating_1999} an approach to reconstruct linear combinations of mixed-partial derivatives of degree $K$ from a family of $K$-jets, which corresponds exactly to our situation and is given by
\begin{equation}
  \label{eq:ttc_general}
  \textstyle % Comment out this line if we have enough space
  \left<
    \partial^{K} f(\vx_0),
    \vv_{n_1}^{\otimes p_1}
    \otimes \ldots \otimes
    \vv_{n_P}^{\otimes p_P}
  \right>
  = \sum_{\underset{\vq \in \mathbb{N}^P}{|\vq| = K}}
  \gamma_{\vp \vq}
  \frac{1}{K!}
  \left<
    \partial^{K}\vf(\vx_0),
      \left(\sum_{p=1}^P \vv_{n_p} q_p\right)^{\otimes K}
  \right>
\end{equation}
\todo{Felix@Tim: I find the notation confusing: Do we first sum, then take the tensor product? If not, we need to set parentheses.
Tim: this is excalty what we are doing. First sum, then product. We might use $(...)^{\otimes K}$}
in our notation.
\Cref{sec:appendix_ttc} provides supplementary details for the multi-index notation and the coefficients in the formula.
From this reconstruction we conclude that \cref{eq:sums-k-directional} can be rewritten as
\begin{align} \label{eq:ttc_general_operator}
  \textstyle % Comment out this line if we have enough space
  \sum_{n_1=1}^{N_1} \dots \sum_{n_P=1}^{N_P}
  \sum_{\underset{\vq \in \mathbb{N}^P}{|\vq| = K}}
  \gamma_{\vp \vq}
  \frac{1}{K!}
  \left<
  \partial^{K}\vf(\vx_0),
  \otimes_{k=1}^K
  \sum_{p = 1}^P \vv_{n_p} q_p
  \right>.
\end{align}
The authors of \cite{griewank_evaluating_1999} recognized that the coefficients $\gamma_{\vp\vq}/K!$ only depend on the problem structure, i.e., on $K$, $P$ and $\vp$ and not on the function $\vf$ and the directions $\vv_{n_k}$.
Therefore, we can pull out the inner sum as well as the coefficients to obtain the expression
\begin{equation}\label{eq:ttc-general}
  \textstyle % Comment out this line if we have enough space
  \sum_{\underset{\vq \in \mathbb{N}^P}{|\vq| = K}}
  \gamma_{\vp \vq}
  \frac{1}{K!}
  \sum_{n_1=1}^{N_1} \dots \sum_{n_P=1}^{N_P}
  \left<
    \partial^{K}\vf(\vx_0),
    \otimes_{k=1}^K
    \sum_{p = 1}^P \vv_{n_p} q_p
  \right>.
\end{equation}
Furthermore, we can exploit symmetries in $\gamma_{\vp\vq}$ that are often inherent to the differential operators under consideration as illustrated in an example below.

To summarize, we have been able to express a general differential operator given by \cref{eq:sums-k-directional} with arbitrary mixed-partial derivatives through a family of $K$-jets.
Thus, it can be efficiently computed via the collapsed Taylor-mode AD propagating different $K$-jets with directions $\sum_{p} \vv_{n_p}q_p$ instead of depending on expensive nested derivative calls.
It is important to emphasize that, in contrast to nesting derivative calls and other approaches (see \cref{sec:appendix_ttc_other_methods} for a brief overview), our method allows for distributing the computations between parallel tasks, since all jets are independent of each other.

\begin{wrapfigure}[20]{r}{0.4\textwidth}
  \centering
  \vspace{-2ex}
  \input{figures/ttc_bilaplacian}
  \vspace{-2ex}
  \caption{\textbf{Illustration of \Cref{eq:ttc-general} for the Bi-Laplacian}, \ie the jet directions and coefficients for computing the partial derivative $\nicefrac{\partial^4}{\partial^2 x_i \partial^2 x_j}$.
    They correspond to the border of a parallelepiped.}
\end{wrapfigure}

\paragraph{Biharmonic Operator} A prominent differential operator with mixed partial derivatives is represented by the Biharmonic Operator.
Our approach first applies \cref{eq:ttc_general} to the Biharmonic Operator \cref{eq:biharm} with the parameters selected as stated above.
The result is shown in \cref{eq:ttc_for_biharm} in the appendix and the coefficients are stated in Figure~\ref{tab:ttc_biharm_coeffs}.

As mentioned before, the coefficients $\gamma_{\vp \vq}$ capture the symmetric structure of the differential operator.
A close look into the definition \cref{eq:ttc_coeff} shows the equality of the coefficients for $\vq = (4,0)$ with $\vq = (0, 4)$ and $\vq = (3, 1)$ with $\vq = (1, 3)$.
Exploiting those symmetries, the formula boils down to \cref{eq:ttc_for_biharm_2}.
Furthermore, the second and third sums contain the jets of the first sum.
This is exploited in \cref{eq:ttc_for_biharm3}.
The last term can be further simplified by leveraging the symmetry of the direction $2 \ve_i + 2 \ve_j$.
This simplification leads to the final efficient jet-based formula as stated in \cref{eq:ttc_for_biharm_final} for the Biharmonic Operator that we use for our numerical results.
The final equation is based on $D + D(D-1) + \frac{D(D-1)}{2}$ different $4$-jets, where once more we can collapse the highest coefficients.
Hence, our formula for the Biharmonic Operator propagates $1 + 3D + 1 + 1 + 3 D(D-1) + 1 + 1 + 3 \frac{D(D-1)}{2} + 1 = 9\frac{D^2}{2} - 3\frac{D}{2} + 6$ vectors through every node in the computational graph, when computed the proposed collapsed Taylor-mode AD.
As before, all jets can be computed in parallel, which provides further benefits for the run-time complexity.

The above procedure describes a generic approach for computing linear differential operators with Taylor-Mode AD that can be combined with our proposed collapsing method.
% We demonstrated it on the Biharmonic Operator, as it is a commonly used operator of degree greater than two.
We also considered other proposed approaches, which underperform our contribution.
For a comparison, see \cref{sec:appendix_ttc_other_methods}.
It has to be remarked that, because of its special structure, there is another efficient scheme for computing the Biharmonic Operator that nests twice the Laplacian.
In the appendix, we briefly discuss this approach in combination with our collapsed Taylor-mode AD.
However, our focus here was to show that our idea applies to other differential operators, therefore making it a relevant contribution.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
