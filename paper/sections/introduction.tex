\todo{Biharmonic Operator instead of Bi-laplace!}\todo{Make clear that we consider PDE operators that consist of sums}Using neural networks to learn functions constrained by physical laws is a popular trend in scientific machine learning \cite{carleo2017solving, pfau2020ab, hermann2020deep, karniadakis2021physics, raissi2019physics, hu2023hutchinson, sun2020global}.
Typically, the physics is encoded through partial differential equations (PDEs) that the neural net must satisfy.
The associated loss functions require evaluating differential operators \wrt the network's input, rather than weights.
Evaluating differential operators remains a computational challenge, especially if they contain high-order derivatives.

\paragraph{Computing PDE operators.} Two important fields that require the evaluation of PDE operators are variational Monte-Carlo (VMC) simulations and physics-informed neural networks (PINNs).
VMC employs neural networks as an ansatz for the Schr\"odinger equation \cite{carleo2017solving, pfau2020ab, hermann2020deep} and demands computing the net's Laplacian, i.e., the Hessian trace usually in a non-differentiable fashion.
PINNs represent PDE solutions as a neural network and train the network via minimizing the residuals of the governing equations \cite{raissi2019physics, karniadakis2021physics}. For instance, Kolmogorov-type equations—including the Fokker-Planck and Black-Scholes equation—requires the differentiable evaluation of weighted second-order derivatives on high-dimensional spatial domains \cite{hu2023hutchinson, sun2024dynamical}. For PINN applications in elasticity, the Biharmonic operator \cite{vahab_physics-informed_2022} requires the computation of fourth-order derivatives, making it a commonly considered a benchmarking problem for computing high-order derivatives in PINNs \cite{hu2023hutchinson, vikas_biharm, shi2024stochastic}.

\paragraph{Is backpropagation all we need?}
% Alternative: The gap between theory and practice
Although nesting first-order automatic differentiation (AD) to compute high-order derivatives scales poorly, this approach is common practice.
\todo{Felix, M: still needs citation.}
A promising alternative to nested backpropagation, especially for higher-order derivatives is \emph{Taylor-mode AD}~\cite[or simply \emph{Taylor-mode},][\S13]{griewank2008evaluating}, introduced to the machine learning community in \citeyear{bettencourt2019taylor} and the JAX ecosystem in \cite{bradbury2018jax}.
However, we observe empirically that vanilla Taylor-mode is often not enough to improve upon nested backpropagation: Evaluating the Laplacian of a 5-layer $\tanh$ activated MLP using JAX' \emph{Taylor mode is 50\% slower} than nested backpropagation via computing, then tracing, the Hessian via Hessian-vector products \cite{pearlmutter1994fast,dagreou2024how}. This calls into question the relevance of Taylor mode for computing common PDE operators.

\paragraph{Improved Taylor mode schemes}
However, recent works have successfully demonstrated the potential of modified forward propagation schemes.
For the (weighted) Laplacian, \citet{li2023forward, li2024dof} developed a special forward propagation framework called the \emph{forward Laplacian}.
Relating to our example above, JAX' forward Laplacian \cite{gao2023folx} is roughly twice as fast as nested backpropagation at reduced memory costs.
While the forward Laplacian does not rely on Taylor mode, recent work pointed out a connection \cite{dangel2024kroneckerfactored}; however, it remains unclear if efficient forward schemes can be derived for other differential operators. Another line of work concerns the stochastic approximation of differential operators in high-dimensions \citet{shi2024stochastic}, relying on Taylor mode with suitably sampled random directions. In this work we identify a mechanism to rewrite the computational graph of standard Taylor mode, applicable to general PDE operators and stochastic Taylor mode: Precisely, our contributions are:
\todo{Felix: Make it more explicit that we evaluate Taylor mode in multiple directions and then sum the results. The sum can be pulled inside.}

\begin{figure*}[!t]
  \centering
  \begin{minipage}[b]{0.42\linewidth}
    \centering
    \input{figures/vanilla_taylor_not_enough.tex}

    \caption{\textbf{$\blacktriangle$ Vanilla Taylor mode is not enough to beat nested 1\textsuperscript{st}-order AD.}
      Illustrated for computing the Laplacian of a $\mathrm{tanh}$-activated $50 \!\to\! 768 \!\to\! 768 \!\to\! 512 \!\to\! 512 \!\to\! 1$
      MLP with JAX (+ \texttt{jit}) on GPU (details in \Cref{sec:jax-benchmark}).
      We show how to automatically obtain the specialized forward Laplacian through simple graph transformations of vanilla Taylor mode.
    }\label{fig:vanilla-taylor-not-enough}

    \vspace{0.25ex}
    \caption{\textbf{$\blacktriangleright$ Our collapsed Taylor mode directly propagates the sum of highest degree coefficients.}
      Visualized for propagating four $K$-jets through a $\sR^5 \!\to\! \sR^3 \!\to\! \sR$ function ($K=2$ yields the forward Laplacian).
      \Cref{sec:background} introduces the notation.}\label{fig:visual-abstract}
  \end{minipage}
  \hfill
  \begin{minipage}[b]{0.57\linewidth}
    \centering
    \input{figures/visual_abstract.tex}
  \end{minipage}
\end{figure*}

\begin{enumerate}[leftmargin=0.5cm]
\item \textbf{We propose optimizing standard Taylor mode by collapsing the highest Taylor coefficients} by directly propagating their sum, rather than propagating then summing (\cref{fig:visual-abstract}). This recovers the forward Laplacian and is applicable to randomized Taylor mode. Furthermore, relying on a result of Griewank et al.\, \cite{griewank_evaluating_1999}, we show how to transform general PDE operators into a form amenable to our setting.

%and, relying on [...] can be generalized to 

  %Collapsing standard Taylor mode for the Laplacian yields the forward Laplacian \cite{li2023forward}, but we show that this optimization is applicable to many other differential operators, and stochastic Taylor mode \cite{shi2024stochastic}.
  \todo{Felix: Phrase this so that it becomes clear that this is a theoretical contribution.}

\item \textbf{We show how to collapse standard Taylor mode by simple graph rewrites based on linearity.}
  This leads to a clean separation of concepts:
  Users can build their computational graph using standard Taylor mode, then use graph rewrites to collapse it.
  Due to the simple nature of our proposed rewrites, this feature could easily be absorbed into the just-in-time (JIT) compilation of ML frameworks, without introducing a new interface.

\item \textbf{We empirically demonstrate the performance improvements of collapsed Taylor mode.} We introduce a Taylor mode library for PyTorch \cite{paszke2019pytorch} that realizes the graph simplifications with \texttt{torch.fx} \cite{reed2022torch}. Furthemore, we show that collapsing Taylor mode yields the theoretically expected improvements in run time and memory consumption. Due to the simple nature of our proposed rewrites, this feature could easily be integrated into the just-in-time (JIT) compilation of ML frameworks, without introducing a new interface.

\end{enumerate}

Our work takes an important step towards the broader adoption of Taylor mode as viable alternative to backpropagation for computing PDE operators, while being as easy to use. 

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
