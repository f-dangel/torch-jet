\documentclass{article}

% use numbers for citations to save space
\PassOptionsToPackage{numbers, compress}{natbib}

% The authors should use one of these tracks.
% Before accepting by the NeurIPS conference, select one of the options below.
% 0. "default" for submission
% either empty (for submission), 'preprint', or 'final'
\def\status{main}
\usepackage[\status]{neurips_2025}
% the "default" option is equal to the "main" option, which is used for the Main Track with double-blind reviewing.
% 1. "main" option is used for the Main Track
%  \usepackage[main]{neurips_2025}
% 2. "position" option is used for the Position Paper Track
%  \usepackage[position]{neurips_2025}
% 3. "dandb" option is used for the Datasets & Benchmarks Track
 % \usepackage[dandb]{neurips_2025}
% 4. "creativeai" option is used for the Creative AI Track
%  \usepackage[creativeai]{neurips_2025}
% 5. "sglblindworkshop" option is used for the Workshop with single-blind reviewing
 % \usepackage[sglblindworkshop]{neurips_2025}
% 6. "dblblindworkshop" option is used for the Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop]{neurips_2025}

% After being accepted, the authors should add "final" behind the track to compile a camera-ready version.
% 1. Main Track
 % \usepackage[main, final]{neurips_2025}
% 2. Position Paper Track
%  \usepackage[position, final]{neurips_2025}
% 3. Datasets & Benchmarks Track
 % \usepackage[dandb, final]{neurips_2025}
% 4. Creative AI Track
%  \usepackage[creativeai, final]{neurips_2025}
% 5. Workshop with single-blind reviewing
%  \usepackage[sglblindworkshop, final]{neurips_2025}
% 6. Workshop with double-blind reviewing
%  \usepackage[dblblindworkshop, final]{neurips_2025}
% Note. For the workshop paper template, both \title{} and \workshoptitle{} are required, with the former indicating the paper title shown in the title and the latter indicating the workshop title displayed in the footnote.
% For workshops (5., 6.), the authors should add the name of the workshop, "\workshoptitle" command is used to set the workshop title.

% "preprint" option is used for arXiv or other preprint submissions
 % \usepackage[preprint]{neurips_2025}

\input{preamble/custom_early.tex}
\input{preamble/neurips_2025.tex}
% follow DL notation from the Goodfellow book
\input{preamble/goodfellow.tex}
\input{preamble/custom.tex}
\input{preamble/metadata.tex}

\definecolor{darkgreen}{rgb}{0,0.6,0}
\newcommand{\AW}[1]{\textcolor{darkgreen}{#1}}
\newcommand{\colorcTM}{tab-green}
\newcommand{\colorTM}{tab-orange}
\newcommand{\colorcTMname}{green}

\begin{document}

\maketitle

\begin{abstract}
  Computing partial differential equation (PDE) operators of high-dimensional functions via nested backpropagation becomes prohibitively expensive in time and memory, severely restricting their utility in scientific machine learning.
  % As an alternative, Taylorâ€‘mode AD computes derivatives in a single forward pass but backpropagation remains the de-facto standard in many implementations.
  Recent advances, such as the forward Laplacian and randomized Taylor mode, propose enhanced forward mode automatic differentiation (AD) schemes
  to address this challenge. We introduce a modified version of Taylor mode AD that ``collapses'' highest-order derivatives by rewriting
  the computational graph, and demonstrate how to exploit this optimization for general PDE operators and randomized Taylor mode.
  %\todo{Felix: This sentence is okay but not 100\% accurate. We do not unify the schemes. We show (i) that standard Taylor mode can be collapsed (e.g. yielding the forward Laplacian) and (ii) that this collapsing can also be applied to STDE).}
  Empirical evaluations for Laplacian and Bi-Laplacian operators confirm the reduced runtime and memory requirements compared to standard Taylor mode AD.
  Our graph-based modifications simply require the propagation of a sum up the computational graph, hence we argue that it could (or should) be performed by the just-in-time compiler in modern machine learning frameworks.
  Our preliminary experiments achieve promising, fully automated speed-ups, which we believe can easily be integrated into automatic differentiation libraries.


%Recent developments in the field of physically informed neural networks have been concerned with the efficient computation of the involved, often higher-order, differential operators. While the classical way of nesting the derivative computations is seen to be inefficient, various methods were investigated to tackle this bottleneck. Besides the approximation of the operators by stochastic sampling, using modified automatic differentiation methods has become a field of interest. To accelerate the evaluation of differential operators, we explore automated computational graph simplifications based on the concept of linearity on top of Taylor-Mode AD to exploit inherent structure of common differential operators. We show that the required Taylor coefficients can first be summed, then propagated, which reduces the overall computational cost, yielding our proposed method collapsed Taylor-Mode.
%These simplifications can be used for many common differential operators like the Laplacian.
%In addition, we combined collapsed Taylor-Mode with a result of the automatic differentiation community to be able to leverage our approach when the operator is not directly representable through Taylor-Mode AD. Our approach can be naturally combined with stochastic approaches and extends previous modified AD schemes while being able to handle operators with derivatives of arbitrary order that were difficult, if not impossible, to evaluate using previous approaches.

%Due to the simplicity of our graph modification (propagating a sum up a computational graph), we argue that it could (or should) be performed by the just-in-time (\texttt{jit}) compiler in machine learning frameworks. Our preliminary experiments achieve promising, fully automated, speed-ups, which we believe can easily be integrated into automatic differentiation libraries.
\end{abstract}

\section{Introduction}\label{sec:introduction}
\input{sections/introduction.tex}

\section{Background: Introduction to Taylor-Mode AD}\label{sec:background}
\input{sections/background.tex}

\section{Collapsing Taylor Mode AD}\label{sec:methodology}
\input{sections/method}

\section{Implementation \& Experiments}\label{sec:experiments}
\input{sections/experiments.tex}

\section{Conclusion}\label{sec:conclusion}
\input{sections/conclusion.tex}

\input{sections/acknowledgements.tex}

\bibliography{references}
\bibliographystyle{icml2024.bst}

\clearpage
\appendix

% Label appendix equations as (A1), (B10) etc.
\renewcommand\theequation{\thesection\arabic{equation}}
\renewcommand\thefigure{\thesection\arabic{figure}}
\renewcommand\thetable{\thesection\arabic{table}}

\section{Visual Tour: From Function to Collapsed Taylor Mode AD}\label{sec:appendix-visual-tour}
\input{figures/interface_overview}
\clearpage

\section{Graph Simplifications}
\input{sections/graph_simplifications}
\clearpage

\section{Exploiting Linearity to Collapse Taylor Mode AD}
\input{sections/appendix_linearity_for_taylor}

\section{Fa\`a Di Bruno Formula Cheat Sheet}\label{sec:faa-di-bruno-cheatsheet}
\input{sections/faa_di_bruno}

\section{Supplementary Material for "Collapsed Taylor Mode for Arbitrary Mixed Partial Derivatives"}\label{sec:appendix_ttc}
\input{sections/appendix_ttc}

\section{PyTorch benchmark}
\label{sec:pytorch-benchmark}
\input{sections/appendix_torch}

\section{JAX benchmark}
\label{sec:jax-benchmark}
\input{sections/appendix_jax}

\newpage
\section*{NeurIPS Paper Checklist}
\input{sections/checklist.tex}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
