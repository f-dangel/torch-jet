We have to justify how our approach goes beyond the forward Laplacian.
One way is that we can compute a wider family of second-order derivatives (weighted sums).
Another one is that our approach is compatible with techniques to scale to extremely high dimensions (randomization).

Our contributions are the following:
\begin{enumerate}
    \item We describe `collapsed' Taylor mode which allows to collapse the highest Taylor coefficients during the forward propagation whenever we compute a weighted sum of said derivatives.
    This trick has first been used in the `forward Laplacian' framework, which however does not rely on Taylor mode.
    Our collapsed Taylor mode improves over it in two ways.
    First, it separates the trick into using standard Taylor mode, combined with simple graph rewrites based on linearity.
    Second, it generalizes to other differential operators, and can also be applied to recently proposed randomization techniques for Taylor mode.
    
    \item This leads to a conceptually clean separation of concepts, allowing for a simple implementation that can handle all scenarios and does not require manual implementations for each differential operator.
    To demonstrate this, we build a Taylor mode library for PyTorch and equip it with the necessary graph simplifications using the \texttt{torch.fx} library, and show how collapsed Taylor mode accelerates the computation of various differential operators.

    \item Ideally, we find specially structured differential operators for which our trick is especially suited, and classify them.
    One example is the trace of derivative tensors of arbitrary order, although this seems to be an operator that is not practically relevant.
\end{enumerate}

Next tasks:
\begin{enumerate}
    \item @Felix: Run experiments with randomized Laplacian to demonstrate compatibility with randomization.
    
    \item @Felix: Run experiments with the weighted  sum of second-order derivatives with $\mC$ PSD to demonstrate we can go beyond the `classical' forward Laplacian
    
    \item @Felix: Extend the graph simplifications to work with weighted sum of second-order derivatives where $\mC$ is indefinite.
    For this, we need to compute two collapsed 2-jets which share some computation (both use the same value on which the function is evaluated. They can therefore share this computation, as well as the derivatives).
    This will require some form of rudimentary common sub-expression elimination.

    \item @Felix [vague]: Think about an example where 
\end{enumerate}
\newpage

\begin{figure}[t]
  \centering
\includegraphics*{../jet/exp/exp01_benchmark_laplacian/figures/architecture_tanh_mlp_768_768_512_512_1_device_cuda_dim_50_name_dangel2024kroneckerfactored_vary_batch_size.pdf}
\caption{ \textbf{Computing the Laplacian with collapsed Taylor mode is faster \emph{and} uses less memory than nested first-order AD; naive Taylor mode is competitive in time but uses more memory.}
  We compare different strategies for computing Laplacians for a $50\to 768 \to 768 \to 512 \to 512 \to 1$ MLP with tanh activations for varying batch sizes on an NVIDIA A40 GPU.
  Solid markers correspond to computing differentiable Laplacians (\eg used in PINNs), opaque markers compute a non-differentiable Laplacian (\eg used in VMC).
  Averaged over all batch sizes, \emph{our collapsed Taylor mode uses only 80\% memory of nested first-order AD (70\% in the non-differentiable setting), and 50\% of time}.
  In contrast, naive Taylor mode uses 120\% memory of nested first-order AD (70\% in the non-differentiable setting) and 95\% of time.
}
  \label{fig:benchmark-laplacians}
\end{figure}

\begin{figure}
  \centering
\includegraphics*{../jet/exp/exp01_benchmark_laplacian/figures/architecture_tanh_mlp_768_768_512_512_1_batch_size_2048_device_cuda_dim_50_distribution_normal_name_dangel2024kroneckerfactored_vary_num_samples.pdf}
\caption{\textbf{Collapsed Taylor mode is compatible with randomization.}
  We show the performance evaluation of the randomized Laplacian with identical setup \Cref{fig:benchmark-laplacians}, but we fix the batch size to be $2048$ and vary the number of samples $S$ used by the Monte-Carlo estimator.
  We find the same relative speedups and memory reductions of collapsing Taylor mode than for the exact Laplacian (as expected since all expressions for the computational demands look the same, but with $S$ replacing $D$).
  Also compare to \Cref{fig:benchmark-laplacians} and note how for $N=2048$ we can obtain a noisy estimate of the Laplacian at much lower cost when $S < D$.
}\label{fig:benchmark-randomized-laplacians}
\end{figure}


%%% Local Variables:
%%% mode: LaTeX
%%% TeX-master: "../main"
%%% End:
